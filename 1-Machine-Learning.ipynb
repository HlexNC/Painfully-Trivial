{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project: Deutsche Bahn Delays - Automatic Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1:** _Automatically Evaluate Dataset & Generate Algorithm-Selection Graph_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module implements an automatic algorithm selection strategy inspired by\n",
    "the scikit‑learn cheat‑sheet. It evaluates a given dataset to determine if the\n",
    "problem is supervised (and, if so, whether it is classification or regression) or unsupervised (clustering or dimensionality reduction) and builds a decision flow graph that explains every decision made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "from typing import List, Dict, Any, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheatSheetSelector:\n",
    "    \"\"\"\n",
    "    A class that emulates the scikit-learn cheat-sheet decision process.\n",
    "\n",
    "    The steps include:\n",
    "      1. Sample size check.\n",
    "      2. Problem type: supervised (classification or regression) or unsupervised (clustering or dimensionality reduction).\n",
    "      3. For classification: further decisions based on text data, high dimensionality, and sample size.\n",
    "      4. For regression: a decision node for linear vs. non-linear relationships and sample size.\n",
    "      5. For unsupervised: a decision between clustering and dimensionality reduction based on feature count.\n",
    "\n",
    "    The complete decision path is recorded and a graph is built with labeled nodes and edges.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_col: Optional[str] = None,\n",
    "        sample_threshold_small: int = 50,\n",
    "        sample_threshold_medium: int = 10_000,\n",
    "        sample_threshold_large: int = 100_000,\n",
    "        assume_linear: Optional[bool] = True  # For regression branch\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param df: Input DataFrame.\n",
    "        :param target_col: Name of the target column (if supervised). If None, dataset is treated as unlabeled.\n",
    "        :param sample_threshold_small: Minimum samples required (e.g., 50).\n",
    "        :param sample_threshold_medium: Threshold for small vs. medium sample sizes (e.g., 10k).\n",
    "        :param sample_threshold_large: Threshold for medium vs. large sample sizes (e.g., 100k).\n",
    "        :param assume_linear: Assumption for regression problems; if True, use linear methods by default.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.n_samples = len(df)\n",
    "        self.sample_threshold_small = sample_threshold_small\n",
    "        self.sample_threshold_medium = sample_threshold_medium\n",
    "        self.sample_threshold_large = sample_threshold_large\n",
    "        self.assume_linear = assume_linear\n",
    "\n",
    "        # Store the decision path as a list of dicts for text-based debugging/logging\n",
    "        self.decision_path: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Graph to render the decision flow\n",
    "        self.graph = Digraph(name=\"CheatSheetGraph\", comment=\"Sklearn Cheat-Sheet Decision Flow\")\n",
    "        self.graph.attr(rankdir='LR', size='10,5')  # Left-to-right flow\n",
    "        self.node_counter = 0  # To create unique node IDs\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Graph Helpers\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _add_node(self, label: str) -> str:\n",
    "        \"\"\"Create a new node in the graph with an incremental ID.\"\"\"\n",
    "        node_id = f\"node_{self.node_counter}\"\n",
    "        self.graph.node(node_id, label)\n",
    "        self.node_counter += 1\n",
    "        return node_id\n",
    "\n",
    "    def _add_edge(self, from_id: str, to_id: str, edge_label: str = \"\"):\n",
    "        \"\"\"Create an edge in the graph with an optional label.\"\"\"\n",
    "        self.graph.edge(from_id, to_id, label=edge_label)\n",
    "\n",
    "    def _record_decision(self, node_name: str, reason: str):\n",
    "        \"\"\"Record a decision step with its explanation (for text-based output).\"\"\"\n",
    "        self.decision_path.append({\"node_name\": node_name, \"reason\": reason})\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Decision Logic Helpers\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _is_supervised(self) -> bool:\n",
    "        \"\"\"Determine if the dataset is supervised by checking for the target column.\"\"\"\n",
    "        return bool(self.target_col and self.target_col in self.df.columns)\n",
    "\n",
    "    def _is_classification(self) -> bool:\n",
    "        \"\"\"\n",
    "        Heuristic to determine if the target variable indicates a classification problem.\n",
    "        If numeric with few unique values, treat it as classification; otherwise, regression.\n",
    "        \"\"\"\n",
    "        if not self._is_supervised():\n",
    "            return False  # Unlabeled data cannot be classification\n",
    "        target_data = self.df[self.target_col]\n",
    "        if pd.api.types.is_numeric_dtype(target_data):\n",
    "            # If numeric but has few unique values, treat it as classification\n",
    "            return target_data.nunique() <= 10\n",
    "        # Non-numeric (string, categorical) → classification\n",
    "        return True\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Classification Decision\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _decide_algorithm_classification(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Decide on a classification algorithm based on text data, high-dimensionality,\n",
    "        and sample size.\n",
    "\n",
    "        :return: (last_node_id, recommended_algorithm_str)\n",
    "        \"\"\"\n",
    "        # Create a top-level node for classification decisions\n",
    "        classification_node = self._add_node(\"Classification Decision\")\n",
    "\n",
    "        # 1) Check if data is high-dimensional: more features than samples\n",
    "        high_dim_node = self._add_node(\"More features than samples?\")\n",
    "        self._add_edge(classification_node, high_dim_node, \"Start Classification Branch\")\n",
    "\n",
    "        num_features = self.df.shape[1] - 1  # Exclude target column\n",
    "        if num_features > self.n_samples:\n",
    "            self._record_decision(\n",
    "                \"High-dimensional Data\",\n",
    "                f\"{num_features} features > {self.n_samples} samples. Use LinearSVC, Naive Bayes, or L1-regularized LogisticRegression.\"\n",
    "            )\n",
    "            rec_algo = \"LinearSVC, Naive Bayes, or L1-Regularized LogisticRegression\"\n",
    "            # Make a leaf node for final recommendation\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(high_dim_node, final_node, \"YES\")\n",
    "            return final_node, rec_algo\n",
    "        else:\n",
    "            self._add_edge(high_dim_node, high_dim_node, \"NO\")  # loop edge for clarity\n",
    "\n",
    "        # 2) Check if data is text-based\n",
    "        text_node = self._add_node(\"Is your data text-based?\")\n",
    "        self._add_edge(high_dim_node, text_node, \"Next\")\n",
    "\n",
    "        has_text = any(\n",
    "            (self.df[col].dtype == object) and (col != self.target_col)\n",
    "            for col in self.df.columns\n",
    "        )\n",
    "        if has_text:\n",
    "            self._record_decision(\n",
    "                \"Text Data\",\n",
    "                \"Detected text data; Naive Bayes or LogisticRegression are strong baselines.\"\n",
    "            )\n",
    "            rec_algo = \"Naive Bayes or LogisticRegression (text-based)\"\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(text_node, final_node, \"YES\")\n",
    "            return final_node, rec_algo\n",
    "        else:\n",
    "            self._add_edge(text_node, text_node, \"NO\")\n",
    "\n",
    "        # 3) Check sample size for classification\n",
    "        sample_node = self._add_node(f\"Do you have < {self.sample_threshold_large} samples?\")\n",
    "        self._add_edge(text_node, sample_node, \"Next\")\n",
    "\n",
    "        if self.n_samples < self.sample_threshold_large:\n",
    "            self._record_decision(\n",
    "                \"Sample Size (Classification)\",\n",
    "                f\"{self.n_samples} < {self.sample_threshold_large}: SVC, KNeighborsClassifier, or MLPClassifier are possible.\"\n",
    "            )\n",
    "            # Check if we have < medium threshold\n",
    "            small_sample_node = self._add_node(f\"Do you have < {self.sample_threshold_medium} samples?\")\n",
    "            self._add_edge(sample_node, small_sample_node, \"YES\")\n",
    "\n",
    "            if self.n_samples < self.sample_threshold_medium:\n",
    "                self._record_decision(\n",
    "                    \"Small Sample Size\",\n",
    "                    f\"{self.n_samples} < {self.sample_threshold_medium}: SVC (RBF), KNeighborsClassifier, or MLPClassifier.\"\n",
    "                )\n",
    "                rec_algo = \"SVC (RBF), KNeighborsClassifier, or MLPClassifier\"\n",
    "                final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                self._add_edge(small_sample_node, final_node, \"YES\")\n",
    "                return final_node, rec_algo\n",
    "            else:\n",
    "                self._record_decision(\n",
    "                    \"Medium Sample Size\",\n",
    "                    f\"{self.n_samples} between {self.sample_threshold_medium} and {self.sample_threshold_large}: LinearSVC, SGDClassifier, or Naive Bayes.\"\n",
    "                )\n",
    "                rec_algo = \"LinearSVC, SGDClassifier, or Naive Bayes\"\n",
    "                final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                self._add_edge(small_sample_node, final_node, \"NO\")\n",
    "                return final_node, rec_algo\n",
    "        else:\n",
    "            self._record_decision(\n",
    "                \"Large Sample Size (Classification)\",\n",
    "                f\"{self.n_samples} >= {self.sample_threshold_large}: Use scalable methods like SGDClassifier or LinearSVC.\"\n",
    "            )\n",
    "            rec_algo = \"SGDClassifier or LinearSVC (large-scale classification)\"\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(sample_node, final_node, \"NO\")\n",
    "            return final_node, rec_algo\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Regression Decision\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _decide_algorithm_regression(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Decide on a regression algorithm based on the assumption of linearity\n",
    "        and the sample size.\n",
    "\n",
    "        :return: (last_node_id, recommended_algorithm_str)\n",
    "        \"\"\"\n",
    "        regression_node = self._add_node(\"Regression Decision\")\n",
    "\n",
    "        # 1) Check if user assumes linear or non-linear\n",
    "        linear_node = self._add_node(\"Assume linear relationship?\")\n",
    "        self._add_edge(regression_node, linear_node, \"Start Regression Branch\")\n",
    "\n",
    "        if self.assume_linear:\n",
    "            self._record_decision(\n",
    "                \"Linear Relationship Assumed\",\n",
    "                \"Suggest LinearRegression, Ridge, Lasso, or ElasticNet.\"\n",
    "            )\n",
    "            # 2) Check sample size\n",
    "            sample_node = self._add_node(f\"Do you have < {self.sample_threshold_large} samples?\")\n",
    "            self._add_edge(linear_node, sample_node, \"YES\")\n",
    "\n",
    "            if self.n_samples < self.sample_threshold_large:\n",
    "                # Check number of features\n",
    "                n_features = self.df.shape[1] - 1\n",
    "                feature_node = self._add_node(\"Number of features < 10?\")\n",
    "                self._add_edge(sample_node, feature_node, \"YES\")\n",
    "\n",
    "                if n_features < 10:\n",
    "                    self._record_decision(\n",
    "                        \"Few Features\",\n",
    "                        f\"{n_features} features < 10. Lasso or ElasticNet recommended for feature selection + regularization.\"\n",
    "                    )\n",
    "                    rec_algo = \"Lasso or ElasticNet\"\n",
    "                    final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                    self._add_edge(feature_node, final_node, \"YES\")\n",
    "                    return final_node, rec_algo\n",
    "                else:\n",
    "                    self._record_decision(\n",
    "                        \"Many Features\",\n",
    "                        f\"{n_features} features >= 10. Suggest RidgeRegression or LinearRegression.\"\n",
    "                    )\n",
    "                    rec_algo = \"RidgeRegression or LinearRegression\"\n",
    "                    final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                    self._add_edge(feature_node, final_node, \"NO\")\n",
    "                    return final_node, rec_algo\n",
    "            else:\n",
    "                self._record_decision(\n",
    "                    \"Large Sample Size (Regression)\",\n",
    "                    f\"{self.n_samples} >= {self.sample_threshold_large}. Use SGDRegressor for scalability.\"\n",
    "                )\n",
    "                rec_algo = \"SGDRegressor\"\n",
    "                final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                self._add_edge(sample_node, final_node, \"NO\")\n",
    "                return final_node, rec_algo\n",
    "        else:\n",
    "            self._add_edge(linear_node, linear_node, \"NO\")\n",
    "            self._record_decision(\n",
    "                \"Non-linear Relationship Assumed\",\n",
    "                \"Consider SVR (RBF), KNeighborsRegressor, DecisionTreeRegressor, ensembles, or MLPRegressor.\"\n",
    "            )\n",
    "            # Check sample size for non-linear\n",
    "            sample_node = self._add_node(f\"Do you have < {self.sample_threshold_large} samples?\")\n",
    "            self._add_edge(linear_node, sample_node, \"Next\")\n",
    "\n",
    "            if self.n_samples < self.sample_threshold_large:\n",
    "                self._record_decision(\n",
    "                    \"Moderate Sample Size (Regression)\",\n",
    "                    f\"{self.n_samples} < {self.sample_threshold_large}: SVR, DecisionTreeRegressor, ensembles, or MLPRegressor.\"\n",
    "                )\n",
    "                rec_algo = \"SVR, KNeighborsRegressor, DecisionTreeRegressor, or ensemble regressors\"\n",
    "                final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                self._add_edge(sample_node, final_node, \"YES\")\n",
    "                return final_node, rec_algo\n",
    "            else:\n",
    "                self._record_decision(\n",
    "                    \"Large Sample Size (Non-linear Regression)\",\n",
    "                    f\"{self.n_samples} >= {self.sample_threshold_large}: Use SGDRegressor or scalable non-linear methods.\"\n",
    "                )\n",
    "                rec_algo = \"SGDRegressor or scalable non-linear methods\"\n",
    "                final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "                self._add_edge(sample_node, final_node, \"NO\")\n",
    "                return final_node, rec_algo\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Clustering Decision\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _decide_algorithm_clustering(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Decide on a clustering algorithm based on the sample size.\n",
    "\n",
    "        :return: (last_node_id, recommended_algorithm_str)\n",
    "        \"\"\"\n",
    "        clustering_node = self._add_node(\"Clustering Decision\")\n",
    "        self._record_decision(\n",
    "            \"Clustering Path\",\n",
    "            \"Unlabeled data. For clustering, if samples < 10k, KMeans or GMM. Otherwise, MiniBatchKMeans or MeanShift.\"\n",
    "        )\n",
    "\n",
    "        # Create a node for sample size check\n",
    "        sample_node = self._add_node(f\"Do you have < {self.sample_threshold_medium} samples?\")\n",
    "        self._add_edge(clustering_node, sample_node, \"Start Clustering Branch\")\n",
    "\n",
    "        if self.n_samples < self.sample_threshold_medium:\n",
    "            self._record_decision(\n",
    "                \"Small/Moderate Sample Size (Clustering)\",\n",
    "                f\"{self.n_samples} < {self.sample_threshold_medium}. KMeans or GMM recommended.\"\n",
    "            )\n",
    "            rec_algo = \"KMeans or GMM\"\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(sample_node, final_node, \"YES\")\n",
    "            return final_node, rec_algo\n",
    "        else:\n",
    "            self._record_decision(\n",
    "                \"Large Sample Size (Clustering)\",\n",
    "                f\"{self.n_samples} >= {self.sample_threshold_medium}. MiniBatchKMeans or MeanShift recommended.\"\n",
    "            )\n",
    "            rec_algo = \"MiniBatchKMeans or MeanShift\"\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(sample_node, final_node, \"NO\")\n",
    "            return final_node, rec_algo\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Dimensionality Reduction Decision\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _decide_algorithm_dimred(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Decide on a dimensionality reduction algorithm based on the sample size.\n",
    "\n",
    "        :return: (last_node_id, recommended_algorithm_str)\n",
    "        \"\"\"\n",
    "        dimred_node = self._add_node(\"Dimensionality Reduction Decision\")\n",
    "        self._record_decision(\n",
    "            \"Dimensionality Reduction Path\",\n",
    "            \"Unlabeled data with high-dimensional features. If samples < 10k, Isomap or Spectral Embedding; otherwise RandomizedPCA or TruncatedSVD.\"\n",
    "        )\n",
    "\n",
    "        # Create a node for sample size check\n",
    "        sample_node = self._add_node(f\"Do you have < {self.sample_threshold_medium} samples?\")\n",
    "        self._add_edge(dimred_node, sample_node, \"Start DimRed Branch\")\n",
    "\n",
    "        if self.n_samples < self.sample_threshold_medium:\n",
    "            self._record_decision(\n",
    "                \"Small/Moderate Sample Size (DimRed)\",\n",
    "                f\"{self.n_samples} < {self.sample_threshold_medium}. Recommend Isomap or Spectral Embedding.\"\n",
    "            )\n",
    "            rec_algo = \"Isomap or Spectral Embedding\"\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(sample_node, final_node, \"YES\")\n",
    "            return final_node, rec_algo\n",
    "        else:\n",
    "            self._record_decision(\n",
    "                \"Large Sample Size (DimRed)\",\n",
    "                f\"{self.n_samples} >= {self.sample_threshold_medium}. Recommend RandomizedPCA or TruncatedSVD.\"\n",
    "            )\n",
    "            rec_algo = \"RandomizedPCA or TruncatedSVD\"\n",
    "            final_node = self._add_node(f\"Recommended: {rec_algo}\")\n",
    "            self._add_edge(sample_node, final_node, \"NO\")\n",
    "            return final_node, rec_algo\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Main Decision Flow\n",
    "    # -------------------------------------------------------------------------\n",
    "    def run(self) -> Tuple[List[Dict[str, Any]], str]:\n",
    "        \"\"\"\n",
    "        Run the full decision-making process:\n",
    "          1. Check sample size.\n",
    "          2. Determine if data is supervised (labeled) or unsupervised.\n",
    "          3. If supervised, check classification or regression.\n",
    "          4. If unsupervised, check clustering or dimensionality reduction.\n",
    "          5. Return the decision path and the final recommended algorithm.\n",
    "\n",
    "        :return: A tuple containing the decision path and the final recommended algorithm.\n",
    "        \"\"\"\n",
    "        # START: Sample size check\n",
    "        start_id = self._add_node(\"START: Check Sample Size (> 50 samples?)\")\n",
    "\n",
    "        if self.n_samples <= self.sample_threshold_small:\n",
    "            # Not enough data\n",
    "            self._record_decision(\n",
    "                \"Sample Size Check\",\n",
    "                f\"Only {self.n_samples} samples (<= {self.sample_threshold_small}). \"\n",
    "                \"Recommend more data or specialized small-sample methods.\"\n",
    "            )\n",
    "            insufficient_node = self._add_node(\"Insufficient Data\")\n",
    "            self._add_edge(start_id, insufficient_node, \"NO\")\n",
    "            return self.decision_path, (\n",
    "                \"Insufficient data. Consider gathering more data or using small-sample methods.\"\n",
    "            )\n",
    "        else:\n",
    "            self._record_decision(\n",
    "                \"Sample Size Check\",\n",
    "                f\"{self.n_samples} samples (> {self.sample_threshold_small}). Proceeding.\"\n",
    "            )\n",
    "            pass_node = self._add_node(\"Sufficient Samples\")\n",
    "            self._add_edge(start_id, pass_node, \"YES\")\n",
    "\n",
    "        # PROBLEM TYPE: Supervised vs. Unsupervised\n",
    "        supervised_node = self._add_node(\"Do we have a target (labeled data)?\")\n",
    "        self._add_edge(pass_node, supervised_node, \"Next\")\n",
    "\n",
    "        if self._is_supervised():\n",
    "            self._record_decision(\n",
    "                \"Supervised Check\",\n",
    "                \"Target column found → supervised learning.\"\n",
    "            )\n",
    "            # Classification vs. Regression\n",
    "            problem_node = self._add_node(\"Are you predicting a category or a quantity?\")\n",
    "            self._add_edge(supervised_node, problem_node, \"YES (labeled data)\")\n",
    "\n",
    "            if self._is_classification():\n",
    "                self._record_decision(\n",
    "                    \"Problem Type\",\n",
    "                    \"Target appears categorical → classification.\"\n",
    "                )\n",
    "                # Classification branch\n",
    "                last_node_id, recommended_algo = self._decide_algorithm_classification()\n",
    "                self._add_edge(problem_node, last_node_id, \"Predicting Category\")\n",
    "            else:\n",
    "                self._record_decision(\n",
    "                    \"Problem Type\",\n",
    "                    \"Target appears numeric → regression.\"\n",
    "                )\n",
    "                # Regression branch\n",
    "                last_node_id, recommended_algo = self._decide_algorithm_regression()\n",
    "                self._add_edge(problem_node, last_node_id, \"Predicting Quantity\")\n",
    "        else:\n",
    "            self._record_decision(\n",
    "                \"Supervised Check\",\n",
    "                \"No target column → unsupervised learning.\"\n",
    "            )\n",
    "            # Clustering vs. Dimensionality Reduction\n",
    "            unsup_node = self._add_node(\"Clustering or Dimensionality Reduction?\")\n",
    "            self._add_edge(supervised_node, unsup_node, \"NO (unlabeled data)\")\n",
    "\n",
    "            n_features = self.df.shape[1]\n",
    "            if n_features <= 10:\n",
    "                self._record_decision(\n",
    "                    \"Unsupervised Task\",\n",
    "                    f\"{n_features} features (<= 10). Assuming clustering.\"\n",
    "                )\n",
    "                last_node_id, recommended_algo = self._decide_algorithm_clustering()\n",
    "                self._add_edge(unsup_node, last_node_id, \"Clustering\")\n",
    "            else:\n",
    "                self._record_decision(\n",
    "                    \"Unsupervised Task\",\n",
    "                    f\"{n_features} features (> 10). Assuming dimensionality reduction.\"\n",
    "                )\n",
    "                last_node_id, recommended_algo = self._decide_algorithm_dimred()\n",
    "                self._add_edge(unsup_node, last_node_id, \"Dimensionality Reduction\")\n",
    "\n",
    "        return self.decision_path, recommended_algo\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Graph Rendering\n",
    "    # -------------------------------------------------------------------------\n",
    "    def render_graph(self, output_filename: str = \"decision_flow\", view: bool = True):\n",
    "        \"\"\"\n",
    "        Render the decision graph to a file (PDF, PNG, etc.) based on the filename extension.\n",
    "\n",
    "        :param output_filename: Output file name (with .pdf or .png extension).\n",
    "        :param view: Whether to open the file after rendering.\n",
    "        \"\"\"\n",
    "        self.graph.render(filename=output_filename, view=view, cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Example usage (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decision Path ===\n",
      "- Sample Size Check: Only 6 samples (<= 50). Recommend more data or specialized small-sample methods.\n",
      "=== Recommended Algorithm ===\n",
      "Insufficient data. Consider gathering more data or using small-sample methods.\n",
      "\n",
      "=== Unsupervised Decision Path ===\n",
      "- Sample Size Check: Only 6 samples (<= 50). Recommend more data or specialized small-sample methods.\n",
      "=== Recommended Algorithm (Unsupervised) ===\n",
      "Insufficient data. Consider gathering more data or using small-sample methods.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Synthetic example dataset\n",
    "    data = {\n",
    "        'feature1': [1, 2, 3, 4, 5, 6],\n",
    "        'feature2': [\"text\", \"text\", \"text\", \"text\", \"text\", \"text\"],\n",
    "        'y_class': [0, 1, 1, 0, 1, 0]\n",
    "    }\n",
    "    df_example = pd.DataFrame(data)\n",
    "\n",
    "    # Example with a target column (supervised classification)\n",
    "    cheat_sheet = CheatSheetSelector(df_example, target_col=\"y_class\")\n",
    "    decision_path, algo = cheat_sheet.run()\n",
    "    cheat_sheet.render_graph(\"algorithm_decision_flow_example\")  # Generates a PDF/PNG of the decision graph\n",
    "\n",
    "    print(\"=== Decision Path ===\")\n",
    "    for step in decision_path:\n",
    "        print(f\"- {step['node_name']}: {step['reason']}\")\n",
    "    print(\"=== Recommended Algorithm ===\")\n",
    "    print(algo)\n",
    "\n",
    "    # For an unsupervised example, remove the target column:\n",
    "    unsupervised_cheat_sheet = CheatSheetSelector(df_example.drop(columns=[\"y_class\"]))\n",
    "    decision_path_unsup, algo_unsup = unsupervised_cheat_sheet.run()\n",
    "    unsupervised_cheat_sheet.render_graph(\"algorithm_decision_flow_unsup\", view=False)\n",
    "\n",
    "    print(\"\\n=== Unsupervised Decision Path ===\")\n",
    "    for step in decision_path_unsup:\n",
    "        print(f\"- {step['node_name']}: {step['reason']}\")\n",
    "    print(\"=== Recommended Algorithm (Unsupervised) ===\")\n",
    "    print(algo_unsup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:** _Real Dataset Example using kagglehub_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-90bb953726cf>:5: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df_1 = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"DBtrainrides.csv\"\n",
    "df_1 = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"nokkyu/deutsche-bahn-db-delays\",\n",
    "    file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 records of the real dataset:\n",
      "                                  ID line  \\\n",
      "0  1573967790757085557-2407072312-14   20   \n",
      "1    349781417030375472-2407080017-1   18   \n",
      "2  7157250219775883918-2407072120-25    1   \n",
      "3    349781417030375472-2407080017-2   18   \n",
      "4   1983158592123451570-2407080010-3   33   \n",
      "\n",
      "                                                path   eva_nr  category  \\\n",
      "0  Stolberg(Rheinl)Hbf Gl.44|Eschweiler-St.Jöris|...  8000001         2   \n",
      "1                                                NaN  8000001         2   \n",
      "2  Hamm(Westf)Hbf|Kamen|Kamen-Methler|Dortmund-Ku...  8000406         4   \n",
      "3                                         Aachen Hbf  8000404         5   \n",
      "4                            Herzogenrath|Kohlscheid  8000404         5   \n",
      "\n",
      "             station                state    city    zip      long        lat  \\\n",
      "0         Aachen Hbf  Nordrhein-Westfalen  Aachen  52064  6.091499  50.767800   \n",
      "1         Aachen Hbf  Nordrhein-Westfalen  Aachen  52064  6.091499  50.767800   \n",
      "2  Aachen-Rothe Erde  Nordrhein-Westfalen  Aachen  52066  6.116475  50.770202   \n",
      "3        Aachen West  Nordrhein-Westfalen  Aachen  52072  6.070715  50.780360   \n",
      "4        Aachen West  Nordrhein-Westfalen  Aachen  52072  6.070715  50.780360   \n",
      "\n",
      "          arrival_plan       departure_plan       arrival_change  \\\n",
      "0  2024-07-08 00:00:00  2024-07-08 00:01:00  2024-07-08 00:03:00   \n",
      "1                  NaN  2024-07-08 00:17:00                  NaN   \n",
      "2  2024-07-08 00:03:00  2024-07-08 00:04:00  2024-07-08 00:03:00   \n",
      "3  2024-07-08 00:20:00  2024-07-08 00:21:00                  NaN   \n",
      "4  2024-07-08 00:20:00  2024-07-08 00:21:00  2024-07-08 00:20:00   \n",
      "\n",
      "      departure_change  arrival_delay_m  departure_delay_m info  \\\n",
      "0  2024-07-08 00:04:00                3                  3  NaN   \n",
      "1                  NaN                0                  0  NaN   \n",
      "2  2024-07-08 00:04:00                0                  0  NaN   \n",
      "3                  NaN                0                  0  NaN   \n",
      "4  2024-07-08 00:21:00                0                  0  NaN   \n",
      "\n",
      "  arrival_delay_check departure_delay_check  \n",
      "0             on_time               on_time  \n",
      "1             on_time               on_time  \n",
      "2             on_time               on_time  \n",
      "3             on_time               on_time  \n",
      "4             on_time               on_time  \n",
      "\n",
      "=== Decision Path (Real Dataset) ===\n",
      "Sample Size Check: 2061357 samples (> 50). Proceeding.\n",
      "Supervised Check: Target column found → supervised learning.\n",
      "Problem Type: Target appears numeric → regression.\n",
      "Linear Relationship Assumed: Suggest LinearRegression, Ridge, Lasso, or ElasticNet.\n",
      "Large Sample Size (Regression): 2061357 >= 100000. Use SGDRegressor for scalability.\n",
      "=== Recommended Algorithm for Real Dataset ===\n",
      "SGDRegressor\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 5 records of the real dataset:\")\n",
    "print(df_1.head())\n",
    "\n",
    "real_selector = CheatSheetSelector(df_1, target_col=\"arrival_delay_m\")\n",
    "decision_path_real, recommendation_real = real_selector.run()\n",
    "real_selector.render_graph(output_filename=\"real_algorithm_decision_flow\", view=True)\n",
    "print(\"\\n=== Decision Path (Real Dataset) ===\")\n",
    "for step in decision_path_real:\n",
    "    print(f\"{step['node_name']}: {step['reason']}\")\n",
    "print(\"=== Recommended Algorithm for Real Dataset ===\")\n",
    "print(recommendation_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-1b60b19f6027>:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df_2 = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (4,6,31,33,61,62,63,76,79,90,92,94,96,114,115,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  result = read_function(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:         eventid  iyear  imonth  iday approxdate  extended resolution  country  \\\n",
      "0  197000000001   1970       7     2        NaN         0        NaN       58   \n",
      "1  197000000002   1970       0     0        NaN         0        NaN      130   \n",
      "2  197001000001   1970       1     0        NaN         0        NaN      160   \n",
      "3  197001000002   1970       1     0        NaN         0        NaN       78   \n",
      "4  197001000003   1970       1     0        NaN         0        NaN      101   \n",
      "\n",
      "          country_txt  region  ... addnotes scite1 scite2  scite3  dbsource  \\\n",
      "0  Dominican Republic       2  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "1              Mexico       1  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "2         Philippines       5  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "3              Greece       8  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "4               Japan       4  ...      NaN    NaN    NaN     NaN      PGIS   \n",
      "\n",
      "   INT_LOG  INT_IDEO INT_MISC INT_ANY  related  \n",
      "0        0         0        0       0      NaN  \n",
      "1        0         1        1       1      NaN  \n",
      "2       -9        -9        1       1      NaN  \n",
      "3       -9        -9        1       1      NaN  \n",
      "4       -9        -9        1       1      NaN  \n",
      "\n",
      "[5 rows x 135 columns]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"globalterrorismdb_0718dist.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df_2 = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"START-UMD/gtd\",\n",
    "  file_path,\n",
    "  pandas_kwargs={\"encoding\": \"ISO-8859-1\"}\n",
    "  # Provide any additional arguments like\n",
    "  # sql_query or pandas_kwargs. See the\n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Path (Real Dataset) ===\n",
      "Sample Size Check: 181691 samples (> 50). Proceeding.\n",
      "Supervised Check: Target column found → supervised learning.\n",
      "Problem Type: Target appears numeric → regression.\n",
      "Linear Relationship Assumed: Suggest LinearRegression, Ridge, Lasso, or ElasticNet.\n",
      "Large Sample Size (Regression): 181691 >= 100000. Use SGDRegressor for scalability.\n",
      "=== Recommended Algorithm for Real Dataset ===\n",
      "SGDRegressor\n"
     ]
    }
   ],
   "source": [
    "real_selector = CheatSheetSelector(df_2, target_col=\"propvalue\")\n",
    "decision_path_real, recommendation_real = real_selector.run()\n",
    "real_selector.render_graph(output_filename=\"real_algorithm_decision_flow\", view=True)\n",
    "print(\"\\n=== Decision Path (Real Dataset) ===\")\n",
    "for step in decision_path_real:\n",
    "    print(f\"{step['node_name']}: {step['reason']}\")\n",
    "print(\"=== Recommended Algorithm for Real Dataset ===\")\n",
    "print(recommendation_real)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
