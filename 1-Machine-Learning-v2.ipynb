{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12693595",
   "metadata": {},
   "source": [
    "# Predicting Deutsche Bahn Train Delays  \n",
    "## A Reproducible Baseline for Supervised Regression\n",
    "\n",
    "**Objective:** Build a supervised regression model to predict train arrival delays (in minutes) for Deutsche Bahn trains using statistical learning methods.\n",
    "\n",
    "**Target Variable:** `arrival_delay_m` - continuous variable representing delay in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f74e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kagglehub import load_dataset, KaggleDatasetAdapter\n",
    "\n",
    "# Load the Deutsche Bahn delays dataset\n",
    "def load_db_delays() -> pd.DataFrame:\n",
    "    df = load_dataset(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"nokkyu/deutsche-bahn-db-delays\",\n",
    "        \"DBtrainrides.csv\"\n",
    "    )\n",
    "    df[\"departure_plan\"] = pd.to_datetime(df[\"departure_plan\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "df = load_db_delays()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacd99e",
   "metadata": {},
   "source": [
    "<!-- ```\n",
    "print(df.head())\n",
    "\n",
    "                                  ID line  \\\n",
    "0  1573967790757085557-2407072312-14   20   \n",
    "1    349781417030375472-2407080017-1   18   \n",
    "2  7157250219775883918-2407072120-25    1   \n",
    "3    349781417030375472-2407080017-2   18   \n",
    "4   1983158592123451570-2407080010-3   33   \n",
    "\n",
    "                                                path   eva_nr  category  \\\n",
    "0  Stolberg(Rheinl)Hbf Gl.44|Eschweiler-St.JÃ¶ris|...  8000001         2   \n",
    "1                                                NaN  8000001         2   \n",
    "2  Hamm(Westf)Hbf|Kamen|Kamen-Methler|Dortmund-Ku...  8000406         4   \n",
    "3                                         Aachen Hbf  8000404         5   \n",
    "4                            Herzogenrath|Kohlscheid  8000404         5   \n",
    "\n",
    "             station                state    city    zip      long        lat  \\\n",
    "0         Aachen Hbf  Nordrhein-Westfalen  Aachen  52064  6.091499  50.767800   \n",
    "1         Aachen Hbf  Nordrhein-Westfalen  Aachen  52064  6.091499  50.767800   \n",
    "2  Aachen-Rothe Erde  Nordrhein-Westfalen  Aachen  52066  6.116475  50.770202   \n",
    "3        Aachen West  Nordrhein-Westfalen  Aachen  52072  6.070715  50.780360   \n",
    "4        Aachen West  Nordrhein-Westfalen  Aachen  52072  6.070715  50.780360   \n",
    "\n",
    "          arrival_plan       departure_plan       arrival_change  \\\n",
    "0  2024-07-08 00:00:00  2024-07-08 00:01:00  2024-07-08 00:03:00   \n",
    "1                  NaN  2024-07-08 00:17:00                  NaN   \n",
    "2  2024-07-08 00:03:00  2024-07-08 00:04:00  2024-07-08 00:03:00   \n",
    "3  2024-07-08 00:20:00  2024-07-08 00:21:00                  NaN   \n",
    "4  2024-07-08 00:20:00  2024-07-08 00:21:00  2024-07-08 00:20:00   \n",
    "\n",
    "      departure_change  arrival_delay_m  departure_delay_m info  \\\n",
    "0  2024-07-08 00:04:00                3                  3  NaN   \n",
    "1                  NaN                0                  0  NaN   \n",
    "2  2024-07-08 00:04:00                0                  0  NaN   \n",
    "3                  NaN                0                  0  NaN   \n",
    "4  2024-07-08 00:21:00                0                  0  NaN   \n",
    "\n",
    "  arrival_delay_check departure_delay_check  \n",
    "0             on_time               on_time  \n",
    "1             on_time               on_time  \n",
    "2             on_time               on_time  \n",
    "3             on_time               on_time  \n",
    "4             on_time               on_time  \n",
    "``` -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289d1dc",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Loading and Initial Inspection\n",
    "\n",
    "**Reference:** *ISLP Chapter 2 - Statistical Learning*, *Slides 03a - The ML Project (p. 5-6)*\n",
    "\n",
    "In any ML project, we begin with understanding our data structure and distribution. As outlined in the ML project steps (Slide 3), data exploration is the crucial second step after data acquisition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parse datetime columns\n",
    "datetime_cols = [\"departure_plan\", \"arrival_plan\", \"departure_change\", \"arrival_change\"]\n",
    "for col in datetime_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target variable (arrival_delay_m) statistics:\")\n",
    "print(df['arrival_delay_m'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8976854",
   "metadata": {},
   "source": [
    "### Understanding the Target Distribution\n",
    "\n",
    "**Mathematical Foundation:** For regression problems, we assume:\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $Y$ is the response variable (arrival_delay_m)\n",
    "- $X$ represents our predictors\n",
    "- $f$ is the systematic information\n",
    "- $\\epsilon$ is random error with $E(\\epsilon) = 0$ and $Var(\\epsilon) = \\sigma^2$\n",
    "\n",
    "**Reference:** *ISLP Equation 2.1, Slides 02 - Machine Learning Overview*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795aa354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots for comprehensive target analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Histogram of arrival delays\n",
    "axes[0, 0].hist(df['arrival_delay_m'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Arrival Delay (minutes)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Arrival Delays')\n",
    "axes[0, 0].axvline(df['arrival_delay_m'].mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {df[\"arrival_delay_m\"].mean():.1f} min')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot to identify outliers\n",
    "axes[0, 1].boxplot(df['arrival_delay_m'].dropna())\n",
    "axes[0, 1].set_ylabel('Arrival Delay (minutes)')\n",
    "axes[0, 1].set_title('Box Plot of Arrival Delays')\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(df['arrival_delay_m'].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot: Checking Normality')\n",
    "\n",
    "# Log-transformed delays (handling negative values)\n",
    "delay_shifted = df['arrival_delay_m'] + abs(df['arrival_delay_m'].min()) + 1\n",
    "axes[1, 1].hist(np.log(delay_shifted.dropna()), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Log(Arrival Delay + offset)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Log-Transformed Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nTarget Variable Analysis:\")\n",
    "print(f\"Mean delay: {df['arrival_delay_m'].mean():.2f} minutes\")\n",
    "print(f\"Median delay: {df['arrival_delay_m'].median():.2f} minutes\")\n",
    "print(f\"Standard deviation: {df['arrival_delay_m'].std():.2f} minutes\")\n",
    "print(f\"Skewness: {df['arrival_delay_m'].skew():.2f}\")\n",
    "print(f\"Percentage of on-time arrivals: {(df['arrival_delay_m'] == 0).sum() / len(df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b37a8",
   "metadata": {},
   "source": [
    "### Missing Data Analysis\n",
    "\n",
    "**Reference:** *ISLP Section 4.6.6 - Missing Data*\n",
    "\n",
    "Missing data can introduce bias if not handled properly. We need to understand the pattern of missingness before deciding on an imputation strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713972b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data visualization\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(missing_data['Column'][:15], missing_data['Missing_Percentage'][:15])\n",
    "plt.xlabel('Missing Percentage (%)')\n",
    "plt.title('Missing Data by Column')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_data[missing_data['Missing_Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150c272",
   "metadata": {},
   "source": [
    "### Feature Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd738f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types for preprocessing\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "datetime_features = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Datetime features ({len(datetime_features)}): {datetime_features}\")\n",
    "\n",
    "# Sample data inspection\n",
    "print(\"\\nSample of the data:\")\n",
    "print(df[['zip', 'category', 'arrival_plan', 'departure_plan', 'arrival_delay_m']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c02386",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b47d0",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "**Mathematical Foundation:** Feature transformation can be represented as:\n",
    "$$\\phi: \\mathcal{X} \\rightarrow \\mathcal{F}$$\n",
    "\n",
    "where $\\phi$ maps from the original feature space $\\mathcal{X}$ to a new feature space $\\mathcal{F}$.\n",
    "\n",
    "**Reference:** *Slides 03a - First Classifiers (Feature Extraction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6073a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy and select relevant columns\n",
    "df_work = df.copy()\n",
    "\n",
    "# Define columns to keep based on domain knowledge\n",
    "keep_cols = [\n",
    "    \"ID\",  # For group-based CV\n",
    "    \"zip\",\n",
    "    \"category\",\n",
    "    \"arrival_plan\",\n",
    "    \"departure_plan\", \n",
    "    \"arrival_change\",\n",
    "    \"departure_change\",\n",
    "    \"arrival_delay_m\"\n",
    "]\n",
    "\n",
    "df_work = df_work[keep_cols]\n",
    "print(f\"Shape after column selection: {df_work.shape}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_work = df_work.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df_work.shape}\")\n",
    "\n",
    "# Feature engineering function\n",
    "def engineer_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Extract temporal features from datetime columns.\n",
    "    Reference: Domain knowledge suggests time-of-day and day-of-week \n",
    "    patterns in train delays.\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df_feat['arr_hour'] = df_feat['arrival_plan'].dt.hour\n",
    "    df_feat['arr_minute'] = df_feat['arrival_plan'].dt.minute\n",
    "    df_feat['arr_weekday'] = df_feat['arrival_plan'].dt.weekday\n",
    "    df_feat['arr_month'] = df_feat['arrival_plan'].dt.month\n",
    "    df_feat['arr_day'] = df_feat['arrival_plan'].dt.day\n",
    "    \n",
    "    df_feat['dep_hour'] = df_feat['departure_plan'].dt.hour\n",
    "    df_feat['dep_minute'] = df_feat['departure_plan'].dt.minute\n",
    "    df_feat['dep_weekday'] = df_feat['departure_plan'].dt.weekday\n",
    "    \n",
    "    # Calculate deltas (in minutes) - these could be strong predictors\n",
    "    df_feat['arr_change_delta'] = (\n",
    "        (df_feat['arrival_change'] - df_feat['arrival_plan'])\n",
    "        .dt.total_seconds() / 60\n",
    "    ).fillna(0)\n",
    "    \n",
    "    df_feat['dep_change_delta'] = (\n",
    "        (df_feat['departure_change'] - df_feat['departure_plan'])\n",
    "        .dt.total_seconds() / 60\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Peak hour indicators\n",
    "    df_feat['is_morning_peak'] = df_feat['arr_hour'].isin([7, 8, 9]).astype(int)\n",
    "    df_feat['is_evening_peak'] = df_feat['arr_hour'].isin([17, 18, 19]).astype(int)\n",
    "    df_feat['is_weekend'] = (df_feat['arr_weekday'] >= 5).astype(int)\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# Apply feature engineering\n",
    "df_work = engineer_temporal_features(df_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a363037",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split\n",
    "\n",
    "**Mathematical Foundation:** To estimate the test error, we use:\n",
    "$$\\text{Test MSE} = E[(Y - \\hat{f}(X))^2]$$\n",
    "\n",
    "We need independent test data to get an unbiased estimate.\n",
    "\n",
    "**Reference:** *ISLP Section 5.1 - Cross-Validation, Slides 03a (p. 9)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df60a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "feature_cols = [col for col in df_work.columns if col not in ['arrival_delay_m', 'ID']]\n",
    "X = df_work[feature_cols + ['ID']]  # Keep ID for group-based CV\n",
    "y = df_work['arrival_delay_m']\n",
    "\n",
    "# Remove rows with missing target\n",
    "mask = ~y.isna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# First split: 80% temp, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=pd.qcut(y, q=10, duplicates='drop')\n",
    ")\n",
    "\n",
    "# Second split: From temp, create 64% train, 16% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Extract IDs for group-based CV\n",
    "train_ids = X_train['ID']\n",
    "X_train = X_train.drop('ID', axis=1)\n",
    "X_val = X_val.drop('ID', axis=1)\n",
    "X_test = X_test.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762cdbd",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline\n",
    "\n",
    "**Mathematical Foundation:** Standardization transforms features to have zero mean and unit variance:\n",
    "$$z_i = \\frac{x_i - \\bar{x}}{s}$$\n",
    "\n",
    "where $\\bar{x}$ is the sample mean and $s$ is the sample standard deviation.\n",
    "\n",
    "**Reference:** *ISLP Section 6.2 - Ridge Regression and Standardization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "datetime_cols_to_drop = ['arrival_plan', 'departure_plan', 'arrival_change', 'departure_change']\n",
    "X_train = X_train.drop(columns=datetime_cols_to_drop, errors='ignore')\n",
    "X_val = X_val.drop(columns=datetime_cols_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=datetime_cols_to_drop, errors='ignore')\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), \n",
    "         categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor on training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Feature correlation analysis (on a sample for efficiency)\n",
    "X_train_transformed = preprocessor.transform(X_train[:1000])\n",
    "feature_names = (numerical_features + \n",
    "                [f\"{cat}_{val}\" for cat, vals in \n",
    "                 zip(categorical_features, preprocessor.named_transformers_['cat'].categories_) \n",
    "                 for val in vals[1:]])  # drop='first' removes first category\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = pd.DataFrame(X_train_transformed, columns=feature_names).corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='RdBu_r', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix (Sample)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808ece8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5194990",
   "metadata": {},
   "source": [
    "## ML Algorithm and Parameter Exploration\n",
    "\n",
    "### Baseline Models\n",
    "\n",
    "**Mathematical Foundation:** The simplest model predicts the mean:\n",
    "$$\\hat{y} = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$\n",
    "\n",
    "This gives us a performance floor.\n",
    "\n",
    "**Reference:** *ISLP Section 3.1 - Simple Linear Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ac998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Mean predictor\n",
    "mean_delay = y_train.mean()\n",
    "baseline_train_mae = mean_absolute_error(y_train, [mean_delay] * len(y_train))\n",
    "baseline_val_mae = mean_absolute_error(y_val, [mean_delay] * len(y_val))\n",
    "\n",
    "print(f\"Baseline Model (Mean Predictor):\")\n",
    "print(f\"Mean delay prediction: {mean_delay:.2f} minutes\")\n",
    "print(f\"Training MAE: {baseline_train_mae:.2f}\")\n",
    "print(f\"Validation MAE: {baseline_val_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c12ea",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "**Mathematical Foundation:** Linear regression assumes:\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p + \\epsilon$$\n",
    "\n",
    "The coefficients are estimated by minimizing RSS:\n",
    "$$\\text{RSS} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Reference:** *ISLP Chapter 3 - Linear Regression, Slides 02*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3868fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_pipeline.predict(X_train)\n",
    "y_val_pred_lr = lr_pipeline.predict(X_val)\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MAE: {mae:.2f} minutes\")\n",
    "    print(f\"RMSE: {rmse:.2f} minutes\")\n",
    "    print(f\"RÂ²: {r2:.3f}\")\n",
    "    \n",
    "    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n",
    "\n",
    "lr_train_metrics = evaluate_model(y_train, y_train_pred_lr, \"Linear Regression - Training\")\n",
    "lr_val_metrics = evaluate_model(y_val, y_val_pred_lr, \"Linear Regression - Validation\")\n",
    "\n",
    "# Residual analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_val - y_val_pred_lr\n",
    "axes[0].scatter(y_val_pred_lr, residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot - Linear Regression')\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1].scatter(y_val, y_val_pred_lr, alpha=0.5)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')\n",
    "axes[1].set_xlabel('Actual Delays')\n",
    "axes[1].set_ylabel('Predicted Delays')\n",
    "axes[1].set_title('Actual vs Predicted - Linear Regression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f61c6",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "**Mathematical Foundation:** Ridge regression minimizes:\n",
    "$$\\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "where $\\lambda \\geq 0$ is the tuning parameter.\n",
    "\n",
    "**Reference:** *ISLP Section 6.2.1 - Ridge Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33100252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with cross-validation for lambda selection\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Define lambda range (alpha in sklearn)\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RidgeCV(alphas=alphas, cv=5))\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal lambda (alpha): {ridge_pipeline.named_steps['regressor'].alpha_:.4f}\")\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred_ridge = ridge_pipeline.predict(X_train)\n",
    "y_val_pred_ridge = ridge_pipeline.predict(X_val)\n",
    "\n",
    "ridge_train_metrics = evaluate_model(y_train, y_train_pred_ridge, \"Ridge Regression - Training\")\n",
    "ridge_val_metrics = evaluate_model(y_val, y_val_pred_ridge, \"Ridge Regression - Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ba51c",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "**Mathematical Foundation:** Lasso minimizes:\n",
    "$$\\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "The L1 penalty can force coefficients to exactly zero, performing variable selection.\n",
    "\n",
    "**Reference:** *ISLP Section 6.2.2 - The Lasso*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lasso with cross-validation\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42, max_iter=2000))\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal lambda (alpha): {lasso_pipeline.named_steps['regressor'].alpha_:.4f}\")\n",
    "\n",
    "# Get feature importance (non-zero coefficients)\n",
    "lasso_coef = lasso_pipeline.named_steps['regressor'].coef_\n",
    "n_selected = np.sum(lasso_coef != 0)\n",
    "print(f\"Number of selected features: {n_selected} out of {len(lasso_coef)}\")\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred_lasso = lasso_pipeline.predict(X_train)\n",
    "y_val_pred_lasso = lasso_pipeline.predict(X_val)\n",
    "\n",
    "lasso_train_metrics = evaluate_model(y_train, y_train_pred_lasso, \"Lasso Regression - Training\")\n",
    "lasso_val_metrics = evaluate_model(y_val, y_val_pred_lasso, \"Lasso Regression - Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be85164",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "**Mathematical Foundation:** Random Forest combines multiple decision trees:\n",
    "$$\\hat{f}(x) = \\frac{1}{B}\\sum_{b=1}^{B} T_b(x)$$\n",
    "\n",
    "where $T_b$ is the $b$-th tree trained on a bootstrap sample.\n",
    "\n",
    "**Reference:** *ISLP Section 8.2.1 - Random Forests, Slides 08*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with limited hyperparameters for initial exploration\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, max_depth=10, \n",
    "                                       min_samples_split=20, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred_rf = rf_pipeline.predict(X_train)\n",
    "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
    "\n",
    "rf_train_metrics = evaluate_model(y_train, y_train_pred_rf, \"Random Forest - Training\")\n",
    "rf_val_metrics = evaluate_model(y_val, y_val_pred_rf, \"Random Forest - Validation\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = rf_pipeline.named_steps['regressor'].feature_importances_\n",
    "feature_names_all = (numerical_features + \n",
    "                    [f\"{cat}_{val}\" for cat, vals in \n",
    "                     zip(categorical_features, preprocessor.named_transformers_['cat'].categories_) \n",
    "                     for val in vals[1:]])\n",
    "\n",
    "# Top 15 features\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names_all,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Most Important Features - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543366fb",
   "metadata": {},
   "source": [
    "### Cross-Validation Comparison\n",
    "\n",
    "**Mathematical Foundation:** K-fold CV estimate:\n",
    "$$\\text{CV}_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k} \\text{MSE}_i$$\n",
    "\n",
    "**Reference:** *ISLP Section 5.1.3 - k-Fold Cross-Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e486a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models using cross-validation\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': RidgeCV(alphas=alphas, cv=5),\n",
    "    'Lasso': LassoCV(cv=5, random_state=42, max_iter=2000),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, \n",
    "                                          min_samples_split=20, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Use negative MAE for scoring (sklearn convention)\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
    "                               cv=kfold, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    cv_results[name] = -cv_scores  # Convert back to positive MAE\n",
    "    \n",
    "    print(f\"{name} - CV MAE: {-cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})\")\n",
    "\n",
    "# Visualization of CV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(cv_results.values(), labels=cv_results.keys())\n",
    "plt.ylabel('MAE (minutes)')\n",
    "plt.title('Cross-Validation Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dc1ff",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-off Analysis\n",
    "\n",
    "**Mathematical Foundation:** The expected test MSE can be decomposed as:\n",
    "$$E[(Y - \\hat{f}(X))^2] = \\text{Var}(\\hat{f}(X)) + [\\text{Bias}(\\hat{f}(X))]^2 + \\text{Var}(\\epsilon)$$\n",
    "\n",
    "**Reference:** *ISLP Section 2.2.2 - The Bias-Variance Trade-Off*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb225a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning curves to visualize bias-variance trade-off\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves(estimator, title, X, y, cv=5):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    train_scores_std = train_scores.std(axis=1)\n",
    "    val_scores_mean = -val_scores.mean(axis=1)\n",
    "    val_scores_std = val_scores.std(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(f\"Learning Curves - {title}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for best models\n",
    "plot_learning_curves(lr_pipeline, \"Linear Regression\", X_train, y_train)\n",
    "plot_learning_curves(rf_pipeline, \"Random Forest\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2d1bf",
   "metadata": {},
   "source": [
    "### Final Model Selection and Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on validation performance, select best model\n",
    "val_performances = {\n",
    "    'Baseline': baseline_val_mae,\n",
    "    'Linear Regression': lr_val_metrics['mae'],\n",
    "    'Ridge': ridge_val_metrics['mae'],\n",
    "    'Lasso': lasso_val_metrics['mae'],\n",
    "    'Random Forest': rf_val_metrics['mae']\n",
    "}\n",
    "\n",
    "best_model_name = min(val_performances, key=val_performances.get)\n",
    "print(f\"\\nBest model based on validation MAE: {best_model_name}\")\n",
    "print(f\"Validation MAE: {val_performances[best_model_name]:.2f} minutes\")\n",
    "\n",
    "# Train best model on combined train+validation set\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    final_model = rf_pipeline\n",
    "elif best_model_name == 'Linear Regression':\n",
    "    final_model = lr_pipeline\n",
    "elif best_model_name == 'Ridge':\n",
    "    final_model = ridge_pipeline\n",
    "else:\n",
    "    final_model = lasso_pipeline\n",
    "\n",
    "# Refit on full training data\n",
    "final_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Final test set evaluation\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, f\"{best_model_name} - Test Set\")\n",
    "\n",
    "# Summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Model comparison bar chart\n",
    "models_list = list(val_performances.keys())\n",
    "mae_values = list(val_performances.values())\n",
    "axes[0, 0].bar(models_list, mae_values)\n",
    "axes[0, 0].set_ylabel('MAE (minutes)')\n",
    "axes[0, 0].set_title('Model Performance Comparison (Validation Set)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Test set predictions\n",
    "axes[0, 1].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[0, 1].set_xlabel('Actual Delays')\n",
    "axes[0, 1].set_ylabel('Predicted Delays')\n",
    "axes[0, 1].set_title(f'Test Set: Actual vs Predicted - {best_model_name}')\n",
    "\n",
    "# Error distribution\n",
    "errors = y_test - y_test_pred\n",
    "axes[1, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Prediction Error (minutes)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Prediction Errors')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# Performance metrics summary\n",
    "metrics_text = f\"\"\"Final Model: {best_model_name}\n",
    "\n",
    "Test Set Performance:\n",
    "MAE: {test_metrics['mae']:.2f} minutes\n",
    "RMSE: {test_metrics['rmse']:.2f} minutes\n",
    "RÂ²: {test_metrics['r2']:.3f}\n",
    "\n",
    "Baseline MAE: {baseline_val_mae:.2f} minutes\n",
    "Improvement: {(baseline_val_mae - test_metrics['mae'])/baseline_val_mae*100:.1f}%\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes,\n",
    "                fontsize=12, verticalalignment='center')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProject Summary:\")\n",
    "print(f\"We successfully built a {best_model_name} model to predict train delays.\")\n",
    "print(f\"The model achieves a test MAE of {test_metrics['mae']:.2f} minutes,\")\n",
    "print(f\"which is a {(baseline_val_mae - test_metrics['mae'])/baseline_val_mae*100:.1f}% improvement over the baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bea593",
   "metadata": {},
   "source": [
    "<!-- \n",
    "## Conclusions and Next Steps\n",
    "\n",
    "This analysis demonstrates the complete machine learning workflow for predicting train delays:\n",
    "\n",
    "1. **Data Exploration**: We identified that train delays follow a right-skewed distribution with many on-time arrivals.\n",
    "\n",
    "2. **Data Preparation**: We engineered temporal features and handled missing data appropriately.\n",
    "\n",
    "3. **Model Selection**: Through systematic evaluation using cross-validation, we compared multiple algorithms from simple (linear regression) to complex (random forest).\n",
    "\n",
    "**Key Findings**:\n",
    "- Temporal features (hour of day, day of week) are important predictors\n",
    "- The change/update times provided by DB are strong predictors but may not always be available\n",
    "- Non-linear models (Random Forest) generally outperform linear models for this problem\n",
    "\n",
    "**Future Improvements**:\n",
    "1. Feature engineering: weather data, holiday indicators, route-specific patterns\n",
    "2. Advanced models: Gradient Boosting, Neural Networks\n",
    "3. Time series approaches: considering sequential nature of delays\n",
    "4. Ensemble methods: combining multiple models\n",
    "\n",
    "**References**:\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning*\n",
    "- Mayer, M. (2025). *Machine Learning Course Slides*, TH Deggendorf -->"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
