{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "12693595",
      "metadata": {
        "id": "12693595"
      },
      "source": [
        "# Predicting Deutsche Bahn Train Delays  \n",
        "## A Reproducible Baseline for Supervised Regression\n",
        "\n",
        "**Objective:** Build a supervised regression model to predict train arrival delays (in minutes) for Deutsche Bahn trains using statistical learning methods.\n",
        "\n",
        "**Target Variable:** `arrival_delay_m` - continuous variable representing delay in minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5af52a9",
      "metadata": {
        "id": "f5af52a9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fc24fe",
      "metadata": {
        "id": "42fc24fe"
      },
      "source": [
        "## 1. Environment Setup and Imports\n",
        "\n",
        "### Google Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec68fd28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec68fd28",
        "outputId": "01c01dcf-9a94-4f81-fbe5-6d9126c73324"
      },
      "outputs": [],
      "source": [
        "# Check if running in Google Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "    # Install required packages\n",
        "    %pip install pandas numpy matplotlib seaborn scikit-learn scipy kagglehub -q\n",
        "\n",
        "    # Mount Google Drive (optional - for saving results)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set memory-efficient pandas options\n",
        "    import pandas as pd\n",
        "    pd.options.mode.chained_assignment = None\n",
        "    pd.options.display.max_columns = 50\n",
        "\n",
        "else:\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df935b3",
      "metadata": {
        "id": "3df935b3"
      },
      "source": [
        "### Local Setup (Anaconda/Miniconda)\n",
        "\n",
        "For local installation, follow these steps in your terminal:\n",
        "\n",
        "```bash\n",
        "# 1. Install Anaconda or Miniconda\n",
        "# Download from: https://www.anaconda.com/download or https://docs.conda.io/en/latest/miniconda.html\n",
        "\n",
        "# 2. Create a new conda environment\n",
        "conda create -n ml-db-delays python=3.9 -y\n",
        "\n",
        "# 3. Activate the environment\n",
        "conda activate ml-db-delays\n",
        "\n",
        "# 4. Install required packages\n",
        "conda install -c conda-forge pandas numpy matplotlib seaborn scikit-learn scipy jupyter notebook ipykernel -y\n",
        "\n",
        "# 5. Install additional packages via pip\n",
        "pip install kagglehub\n",
        "\n",
        "# 6. Add kernel to Jupyter\n",
        "python -m ipykernel install --user --name ml-db-delays --display-name \"ML DB Delays\"\n",
        "\n",
        "# 7. Launch Jupyter Notebook\n",
        "jupyter notebook\n",
        "\n",
        "# 8. Select the \"ML DB Delays\" kernel when creating/opening the notebook\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1370c4af",
      "metadata": {
        "id": "1370c4af"
      },
      "source": [
        "### Mathematical Foundation\n",
        "As per ITSL Chapter 2.1, we model the relationship between predictors and response as:\n",
        "\n",
        "$$Y = f(X) + \\epsilon$$\n",
        "\n",
        "where:\n",
        "- $Y$ is the response variable (arrival delay in minutes)\n",
        "- $X = (X_1, X_2, ..., X_p)$ represents our $p$ predictors\n",
        "- $f$ is the unknown systematic function we aim to estimate\n",
        "- $\\epsilon$ is random error with $E(\\epsilon) = 0$ and $Var(\\epsilon) = \\sigma^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721512ec",
      "metadata": {
        "id": "721512ec"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f0422e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29f0422e",
        "outputId": "aeae6341-ce51-43d2-8318-ea161c9252e7"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import psutil  # For memory monitoring\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "import sklearn\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV,\n",
        "    KFold, learning_curve, validation_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    make_scorer\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "# Feature selection\n",
        "try:\n",
        "    from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "except:\n",
        "    print(\"mlxtend not available - will use alternative feature selection\")\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "\n",
        "# Kaggle data loading\n",
        "import kagglehub\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "# Memory-efficient pandas settings\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.options.display.max_columns = 50\n",
        "pd.options.display.max_rows = 100\n",
        "\n",
        "# Plotting configuration\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['figure.dpi'] = 100  # Reduce DPI for memory efficiency\n",
        "\n",
        "# Memory monitoring function\n",
        "def check_memory():\n",
        "    \"\"\"Monitor memory usage\"\"\"\n",
        "    if 'psutil' in sys.modules:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        mem_info = process.memory_info()\n",
        "        return f\"Memory Usage: {mem_info.rss / 1024 / 1024 / 1024:.2f} GB\"\n",
        "    else:\n",
        "        return \"psutil not available - install with: pip install psutil\"\n",
        "\n",
        "print(\"All packages imported successfully!\")\n",
        "print(check_memory())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea15c97",
      "metadata": {
        "id": "7ea15c97"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762f97d8",
      "metadata": {
        "id": "762f97d8"
      },
      "source": [
        "## 2. Data Loading and Initial Inspection\n",
        "\n",
        "### The Supervised Learning Workflow (Lecture Slides)\n",
        "\n",
        "Following the supervised learning experiment steps:\n",
        "1. **Training data** → **Preprocessing** → **Feature extraction** → **Feature selection** → **Training**\n",
        "2. **Test data** → **Preprocessing** → **Selected feature extraction** → **Classifier** → **Classification result**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f74e24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29f74e24",
        "outputId": "f1b8c78a-50f5-41ca-be4f-bb71580c3174"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "print(\"Downloading Deutsche Bahn delays dataset...\")\n",
        "path = kagglehub.dataset_download(\"nokkyu/deutsche-bahn-db-delays\")\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "# Find the CSV file\n",
        "file_path = os.path.join(path, 'db_delays.csv')\n",
        "print(f\"\\nLoading data from: {file_path}\")\n",
        "\n",
        "# Load data with optimized settings\n",
        "print(\"\\nLoading dataset...\")\n",
        "df = pd.read_csv(file_path, \n",
        "                 parse_dates=['arrival_plan', 'departure_plan', 'arrival_change', 'departure_change'],\n",
        "                 low_memory=False)\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "print(check_memory())\n",
        "\n",
        "# For extremely large datasets, consider sampling\n",
        "if len(df) > 2000000:  # If more than 2 million rows\n",
        "    print(f\"\\nDataset has {len(df):,} rows. Sampling for manageable processing...\")\n",
        "    df = df.sample(n=min(1000000, len(df)), random_state=42)\n",
        "    print(f\"Working with {len(df):,} sampled rows\")\n",
        "    gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8957a59",
      "metadata": {
        "id": "e8957a59"
      },
      "source": [
        "### Initial Data Quality Assessment\n",
        "\n",
        "As stated in the slides: \"The data quality should be good\" and \"Before doing classification/regression experiments, you should be familiar with the data.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f996b00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f996b00",
        "outputId": "3572850a-ecd8-4e68-f56b-958fbe58ec1f"
      },
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*80)\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIRST 5 ROWS\")\n",
        "print(\"=\"*80)\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE STATISTICS (arrival_delay_m)\")\n",
        "print(\"=\"*80)\n",
        "print(df['arrival_delay_m'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3cdecd",
      "metadata": {
        "id": "8c3cdecd"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0869a8d7",
      "metadata": {
        "id": "0869a8d7"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "### 3.1 Remove Duplicates\n",
        "\n",
        "First step in data cleaning: remove duplicate records to ensure data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b0a7bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "a6b0a7bc",
        "outputId": "49bf704b-b78f-430a-f3b2-eb10fd4122c2"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE REMOVAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "initial_rows = len(df)\n",
        "df = df.drop_duplicates()\n",
        "final_rows = len(df)\n",
        "\n",
        "print(f\"Rows before removing duplicates: {initial_rows:,}\")\n",
        "print(f\"Rows after removing duplicates: {final_rows:,}\")\n",
        "print(f\"Duplicates removed: {initial_rows - final_rows:,} ({(initial_rows - final_rows)/initial_rows*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0255d89e",
      "metadata": {
        "id": "0255d89e"
      },
      "source": [
        "### 3.2 Missing Value Analysis and Treatment\n",
        "\n",
        "Following best practices: analyze missing patterns before deciding on treatment strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26da8ef7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "26da8ef7",
        "outputId": "9fd39260-816d-4744-bbd7-8f5481a6d132"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate missing values\n",
        "missing_stats = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
        "})\n",
        "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "print(missing_stats)\n",
        "\n",
        "# Visualize missing patterns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Heatmap of missing values\n",
        "missing_cols = df.columns[df.isnull().any()].tolist()\n",
        "if len(missing_cols) > 0:\n",
        "    # Sample for visualization\n",
        "    sample_size = min(1000, len(df))\n",
        "    sample_df = df[missing_cols].sample(n=sample_size, random_state=42)\n",
        "    \n",
        "    sns.heatmap(sample_df.isnull(), cbar=True, ax=axes[0], cmap='viridis_r', \n",
        "                yticklabels=False)\n",
        "    axes[0].set_title('Missing Values Pattern (Sample)')\n",
        "    axes[0].set_xlabel('Features')\n",
        "\n",
        "# Bar plot of missing percentages\n",
        "missing_stats.plot(x='Column', y='Missing_Percentage', kind='barh', ax=axes[1], legend=False)\n",
        "axes[1].set_xlabel('Missing Percentage (%)')\n",
        "axes[1].set_title('Missing Values by Feature')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c3bc5b",
      "metadata": {
        "id": "08c3bc5b"
      },
      "source": [
        "**Analysis:** The feature analysis reveals several important patterns:\n",
        "1. Train category significantly impacts delays, with long-distance trains showing higher average delays\n",
        "2. Clear temporal patterns exist, with peak hours showing increased delays\n",
        "3. Strong correlation between departure and arrival delays suggests departure delay is a key predictor\n",
        "4. Geographic variations indicate location-based features may improve predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c347077b",
      "metadata": {
        "id": "c347077b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b39ca113",
      "metadata": {
        "id": "b39ca113"
      },
      "source": [
        "## 4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014a2112",
      "metadata": {
        "id": "014a2112"
      },
      "source": [
        "### Outlier Detection and Treatment\n",
        "Following the slides on \"Outliers, cross validation, resampling\", we implement robust outlier detection.\n",
        "\n",
        "#### Mathematical Framework for Outlier Detection\n",
        "We use two approaches as discussed in the lecture:\n",
        "\n",
        "1. **IQR Method**: Values outside $[Q_1 - 1.5 \\times IQR, Q_3 + 1.5 \\times IQR]$ are considered outliers\n",
        "2. **Z-Score Method**: Values with $|z| > 3$ where $z = \\frac{x - \\mu}{\\sigma}$ are considered outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202c7425",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "202c7425",
        "outputId": "0db345ef-d7f7-46ec-8e50-24c9a9405f10"
      },
      "outputs": [],
      "source": [
        "def detect_outliers(df, column, method='IQR'):\n",
        "    \"\"\"\n",
        "    Detect outliers using IQR or Z-score method\n",
        "    Reference: Slides on Outlier Detection\n",
        "    \"\"\"\n",
        "    if method == 'IQR':\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
        "    elif method == 'Z-score':\n",
        "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
        "        outliers = z_scores > 3\n",
        "\n",
        "    return outliers, lower_bound if method == 'IQR' else -3, upper_bound if method == 'IQR' else 3\n",
        "\n",
        "# Analyze outliers in target variable\n",
        "outliers_iqr, lower_iqr, upper_iqr = detect_outliers(df, 'arrival_delay_m', method='IQR')\n",
        "outliers_zscore, _, _ = detect_outliers(df, 'arrival_delay_m', method='Z-score')\n",
        "\n",
        "print(\"Outlier Analysis:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Outliers detected by IQR method: {outliers_iqr.sum()} ({outliers_iqr.sum()/len(df)*100:.2f}%)\")\n",
        "print(f\"Outliers detected by Z-score method: {outliers_zscore.sum()} ({outliers_zscore.sum()/len(df)*100:.2f}%)\")\n",
        "print(f\"\\nIQR bounds: [{lower_iqr:.2f}, {upper_iqr:.2f}] minutes\")\n",
        "\n",
        "# Visualize outlier impact\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi=80)\n",
        "\n",
        "# Original distribution\n",
        "sample_viz = df.sample(min(50000, len(df)))\n",
        "axes[0].hist(sample_viz['arrival_delay_m'].dropna(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[0].axvline(upper_iqr, color='red', linestyle='--', label='IQR Upper Bound')\n",
        "axes[0].set_xlabel('Arrival Delay (minutes)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Original Distribution with Outlier Threshold')\n",
        "axes[0].set_xlim(-50, 200)\n",
        "axes[0].legend()\n",
        "\n",
        "# Distribution after outlier capping\n",
        "df['arrival_delay_m_capped'] = df['arrival_delay_m'].clip(lower=lower_iqr, upper=upper_iqr)\n",
        "axes[1].hist(sample_viz['arrival_delay_m'].clip(lower=lower_iqr, upper=upper_iqr),\n",
        "             bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "axes[1].set_xlabel('Arrival Delay (minutes)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Distribution after Outlier Capping')\n",
        "axes[1].set_xlim(-50, 200)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOutlier Treatment Decision: Cap delays at [{lower_iqr:.0f}, {upper_iqr:.0f}] minutes\")\n",
        "\n",
        "# Cleanup\n",
        "del sample_viz, outliers_iqr, outliers_zscore\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9f4b37",
      "metadata": {
        "id": "df9f4b37"
      },
      "source": [
        "### Missing Value Treatment Strategy\n",
        "\n",
        "Based on the analysis:\n",
        "1. Remove rows with missing target variable (arrival_delay_m)\n",
        "2. Remove rows with missing critical features\n",
        "3. Impute remaining missing values appropriately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b94d98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44b94d98",
        "outputId": "ec27a3f3-7dbf-406a-843e-db67092e6bce"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUE TREATMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Remove rows with missing target variable\n",
        "initial_len = len(df)\n",
        "df = df.dropna(subset=['arrival_delay_m'])\n",
        "print(f\"Removed {initial_len - len(df):,} rows with missing target values\")\n",
        "\n",
        "# 2. Remove rows with missing critical features\n",
        "critical_features = ['eva_nr', 'category', 'departure_delay_m', 'long', 'lat']\n",
        "before_critical = len(df)\n",
        "df = df.dropna(subset=critical_features)\n",
        "print(f\"Removed {before_critical - len(df):,} rows with missing critical features\")\n",
        "\n",
        "# 3. Impute remaining missing values\n",
        "# For categorical columns: fill with 'Unknown'\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().any():\n",
        "        df[col] = df[col].fillna('Unknown')\n",
        "        print(f\"Filled {col} with 'Unknown'\")\n",
        "\n",
        "# For numerical columns: fill with median\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "for col in numerical_cols:\n",
        "    if col != 'arrival_delay_m' and df[col].isnull().any():\n",
        "        median_val = df[col].median()\n",
        "        df[col] = df[col].fillna(median_val)\n",
        "        print(f\"Filled {col} with median value: {median_val:.2f}\")\n",
        "\n",
        "print(f\"\\nFinal dataset size after missing value treatment: {len(df):,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec1562a",
      "metadata": {
        "id": "5ec1562a"
      },
      "source": [
        "### 3.3 Feature Engineering\n",
        "\n",
        "Create domain-specific features based on the available data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858283dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "858283dc",
        "outputId": "0319f0a0-778e-4703-a4bc-4366482a1325"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extract temporal features\n",
        "df['hour'] = pd.to_datetime(df['departure_plan']).dt.hour\n",
        "df['day_of_week'] = pd.to_datetime(df['departure_plan']).dt.dayofweek\n",
        "df['month'] = pd.to_datetime(df['departure_plan']).dt.month\n",
        "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
        "\n",
        "# Station-based aggregated features\n",
        "print(\"Creating station-based features...\")\n",
        "station_stats = df.groupby('station')['arrival_delay_m'].agg(['mean', 'std', 'count']).reset_index()\n",
        "station_stats.columns = ['station', 'station_avg_delay', 'station_std_delay', 'station_traffic']\n",
        "df = df.merge(station_stats, on='station', how='left')\n",
        "\n",
        "# Category-based aggregated features\n",
        "print(\"Creating category-based features...\")\n",
        "category_stats = df.groupby('category')['arrival_delay_m'].agg(['mean', 'std']).reset_index()\n",
        "category_stats.columns = ['category', 'category_avg_delay', 'category_std_delay']\n",
        "df = df.merge(category_stats, on='category', how='left')\n",
        "\n",
        "print(\"\\nNew features created:\")\n",
        "print(\"- Temporal: hour, day_of_week, month, is_weekend, is_rush_hour\")\n",
        "print(\"- Station-based: station_avg_delay, station_std_delay, station_traffic\")\n",
        "print(\"- Category-based: category_avg_delay, category_std_delay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cae13786",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289902eb",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis\n",
        "\n",
        "### 4.1 Target Variable Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253c3364",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Distribution plot\n",
        "axes[0].hist(df['arrival_delay_m'], bins=100, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Arrival Delay (minutes)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Arrival Delays')\n",
        "axes[0].set_xlim(-50, 200)\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(df['arrival_delay_m'], vert=True)\n",
        "axes[1].set_ylabel('Arrival Delay (minutes)')\n",
        "axes[1].set_title('Box Plot of Arrival Delays')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nTarget Variable Statistics:\")\n",
        "print(df['arrival_delay_m'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6669134b",
      "metadata": {},
      "source": [
        "### 4.2 Feature Correlations\n",
        "\n",
        "As noted in lecture (Feature Engineering slides): \n",
        "\"Correlation matrices are often used to visualize (linear!) dependency among features.\"\n",
        "\n",
        "**Warning**: \"Temptation: Just take the features with best correlation to your goal for your predictions! This is deeply wrong.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a739bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numerical features for correlation analysis\n",
        "numerical_features = ['arrival_delay_m', 'departure_delay_m', 'category', 'eva_nr', \n",
        "                     'zip', 'long', 'lat', 'hour', 'day_of_week', 'month',\n",
        "                     'is_weekend', 'is_rush_hour', 'station_avg_delay', \n",
        "                     'station_std_delay', 'category_avg_delay']\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df[numerical_features].corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
        "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
        "            cbar_kws={\"shrink\": .8})\n",
        "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top correlations with target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP CORRELATIONS WITH TARGET VARIABLE\")\n",
        "print(\"=\"*80)\n",
        "target_corr = corr_matrix['arrival_delay_m'].sort_values(ascending=False)\n",
        "print(target_corr[1:11])  # Exclude self-correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9eced8d",
      "metadata": {},
      "source": [
        "### 4.3 Feature Relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955150a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze key relationships\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Departure delay vs Arrival delay\n",
        "sample_size = min(5000, len(df))\n",
        "sample_indices = np.random.choice(df.index, sample_size, replace=False)\n",
        "axes[0, 0].scatter(df.loc[sample_indices, 'departure_delay_m'], \n",
        "                   df.loc[sample_indices, 'arrival_delay_m'], \n",
        "                   alpha=0.5, s=10)\n",
        "axes[0, 0].set_xlabel('Departure Delay (minutes)')\n",
        "axes[0, 0].set_ylabel('Arrival Delay (minutes)')\n",
        "axes[0, 0].set_title('Departure Delay vs Arrival Delay')\n",
        "axes[0, 0].plot([0, 100], [0, 100], 'r--', alpha=0.5)\n",
        "\n",
        "# 2. Average delay by category\n",
        "category_delays = df.groupby('category')['arrival_delay_m'].agg(['mean', 'std'])\n",
        "category_delays.sort_values('mean').plot(y='mean', kind='barh', ax=axes[0, 1], \n",
        "                                        xerr='std', capsize=3)\n",
        "axes[0, 1].set_xlabel('Average Delay (minutes)')\n",
        "axes[0, 1].set_title('Average Delay by Train Category')\n",
        "\n",
        "# 3. Hourly pattern\n",
        "hourly_pattern = df.groupby('hour')['arrival_delay_m'].agg(['mean', 'std'])\n",
        "axes[1, 0].errorbar(hourly_pattern.index, hourly_pattern['mean'], \n",
        "                    yerr=hourly_pattern['std'], marker='o', capsize=3)\n",
        "axes[1, 0].set_xlabel('Hour of Day')\n",
        "axes[1, 0].set_ylabel('Average Delay (minutes)')\n",
        "axes[1, 0].set_title('Average Delay by Hour of Day')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Weekend effect\n",
        "weekend_comparison = df.groupby('is_weekend')['arrival_delay_m'].apply(list)\n",
        "axes[1, 1].boxplot([weekend_comparison[0][:5000], weekend_comparison[1][:5000]], \n",
        "                   labels=['Weekday', 'Weekend'])\n",
        "axes[1, 1].set_ylabel('Arrival Delay (minutes)')\n",
        "axes[1, 1].set_title('Delay Distribution: Weekday vs Weekend')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f16ebab",
      "metadata": {
        "id": "4f16ebab"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451f2ea3",
      "metadata": {
        "id": "451f2ea3"
      },
      "source": [
        "## 5. Data Splitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f85760eb",
      "metadata": {
        "id": "f85760eb"
      },
      "source": [
        "\n",
        "### Train-Validation-Test Split (60-20-20)\n",
        "\n",
        "Following ML best practices:\n",
        "- **Training set (60%)**: For model fitting\n",
        "- **Validation set (20%)**: For hyperparameter tuning and model selection\n",
        "- **Test set (20%)**: For final unbiased evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefb8a80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefb8a80",
        "outputId": "acf9af68-df48-4d82-fc1b-10332e3726be"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA SPLITTING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define features and target\n",
        "feature_columns = [\n",
        "    # Numerical features\n",
        "    'eva_nr', 'category', 'zip', 'long', 'lat', 'departure_delay_m',\n",
        "    'hour', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour',\n",
        "    'station_avg_delay', 'station_std_delay', 'station_traffic',\n",
        "    'category_avg_delay', 'category_std_delay',\n",
        "    # Categorical features\n",
        "    'station', 'state', 'line'\n",
        "]\n",
        "\n",
        "X = df[feature_columns].copy()\n",
        "y = df['arrival_delay_m'].copy()\n",
        "\n",
        "print(f\"Total samples: {len(X):,}\")\n",
        "print(f\"Number of features: {len(feature_columns)}\")\n",
        "\n",
        "# First split: separate test set (20%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Second split: separate train (60%) and validation (20%) from remaining 80%\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.25, random_state=42, shuffle=True  # 0.25 * 0.8 = 0.2\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verify target distribution is similar across splits\n",
        "print(f\"\\nTarget variable statistics by split:\")\n",
        "print(f\"Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
        "print(f\"Val   - Mean: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
        "print(f\"Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b3b02e",
      "metadata": {
        "id": "62b3b02e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd25162",
      "metadata": {
        "id": "afd25162"
      },
      "source": [
        "## 6. Feature Preprocessing Pipeline\n",
        "\n",
        "### StandardScaler and OneHotEncoder\n",
        "\n",
        "From ISLR Section 4.7.4: \"A good way to handle this problem is to **standardize** the data so that all variables are given a mean of zero and a standard deviation of one.\"\n",
        "\n",
        "**Important**: Fit preprocessing only on training data to avoid data leakage!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17531155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "17531155",
        "outputId": "ef139570-1d6f-4f08-f973-e1db35684702"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPROCESSING PIPELINE SETUP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "# StandardScaler: transforms features to have mean=0 and std=1\n",
        "# OneHotEncoder: creates binary features for each category\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), \n",
        "         categorical_features)\n",
        "    ],\n",
        "    remainder='drop'  # Drop any other columns\n",
        ")\n",
        "\n",
        "# IMPORTANT: Fit preprocessor on training data ONLY\n",
        "print(\"\\nFitting preprocessor on training data...\")\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "feature_names_after_preprocessing = (\n",
        "    numerical_features + \n",
        "    [f\"{cat}_{val}\" for cat, vals in \n",
        "     zip(categorical_features, preprocessor.named_transformers_['cat'].categories_)\n",
        "     for val in vals[1:]]  # drop='first' removes first category\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal features after preprocessing: {len(feature_names_after_preprocessing)}\")\n",
        "print(\"Preprocessing pipeline created and fitted on training data only!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deaa4083",
      "metadata": {
        "id": "deaa4083"
      },
      "source": [
        "**Analysis:** The cross-validation results show consistent performance across folds, indicating stable model estimates. The tree-based model shows lower bias but higher variance compared to linear models, consistent with the bias-variance tradeoff discussed in ITSL Section 2.2.2.\n",
        "\n",
        "### Learning Curves Analysis\n",
        "Following ITSL concepts on model complexity and sample size effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "227b3ce3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "227b3ce3",
        "outputId": "4c17eff5-0349-43ce-fdfb-2e535a43cec8"
      },
      "outputs": [],
      "source": [
        "# Plot learning curves for best two models only\n",
        "def plot_learning_curves(estimator, title, X, y, cv=3,\n",
        "                        train_sizes=np.linspace(0.2, 1.0, 5)):\n",
        "    \"\"\"Memory-efficient learning curves\"\"\"\n",
        "\n",
        "    # Use at most 50k samples\n",
        "    if len(X) > 50000:\n",
        "        indices = np.random.choice(len(X), 50000, replace=False)\n",
        "        X_sample = X.iloc[indices]\n",
        "        y_sample = y.iloc[indices]\n",
        "    else:\n",
        "        X_sample = X\n",
        "        y_sample = y\n",
        "\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator, X_sample, y_sample, cv=cv, n_jobs=-1,\n",
        "        train_sizes=train_sizes,\n",
        "        scoring='neg_mean_absolute_error'\n",
        "    )\n",
        "\n",
        "    return train_sizes, -train_scores, -val_scores\n",
        "\n",
        "# Plot for Linear Regression and Random Forest\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi=80)\n",
        "\n",
        "models_to_plot = [\n",
        "    ('Linear Regression', lr_pipeline),\n",
        "    ('Random Forest', rf_grid_search.best_estimator_)\n",
        "]\n",
        "\n",
        "for idx, (name, model) in enumerate(models_to_plot):\n",
        "    print(f\"Computing learning curves for {name}...\")\n",
        "    train_sizes, train_scores, val_scores = plot_learning_curves(\n",
        "        model, name, X_train, y_train\n",
        "    )\n",
        "\n",
        "    ax = axes[idx]\n",
        "    train_mean = train_scores.mean(axis=1)\n",
        "    train_std = train_scores.std(axis=1)\n",
        "    val_mean = val_scores.mean(axis=1)\n",
        "    val_std = val_scores.std(axis=1)\n",
        "\n",
        "    ax.fill_between(train_sizes, train_mean - train_std,\n",
        "                    train_mean + train_std, alpha=0.1, color=\"r\")\n",
        "    ax.fill_between(train_sizes, val_mean - val_std,\n",
        "                    val_mean + val_std, alpha=0.1, color=\"g\")\n",
        "    ax.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    ax.plot(train_sizes, val_mean, 'o-', color=\"g\", label=\"Validation score\")\n",
        "\n",
        "    ax.set_xlabel(\"Training Set Size\")\n",
        "    ax.set_ylabel(\"MAE (minutes)\")\n",
        "    ax.set_title(f\"Learning Curves - {name}\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff09367",
      "metadata": {
        "id": "9ff09367"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bde9a11",
      "metadata": {
        "id": "0bde9a11"
      },
      "source": [
        "## 7. Feature Selection\n",
        "\n",
        "### Forward Stepwise Selection\n",
        "\n",
        "From ISLR Chapter 6.1.2:\n",
        "\n",
        "**Algorithm 6.2 Forward stepwise selection**\n",
        "1. Let $M_0$ denote the null model, which contains no predictors\n",
        "2. For $k = 0, ..., p-1$:\n",
        "   - Consider all $p-k$ models that augment the predictors in $M_k$ with one additional predictor\n",
        "   - Choose the best among these $p-k$ models, and call it $M_{k+1}$\n",
        "3. Select a single best model from among $M_0, ..., M_p$ using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d67f9aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d67f9aa",
        "outputId": "8cdeb5a6-d823-4de8-f786-b56de9af01da"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SELECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Transform data using fitted preprocessor\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "X_val_transformed = preprocessor.transform(X_val)\n",
        "\n",
        "print(f\"Transformed training data shape: {X_train_transformed.shape}\")\n",
        "\n",
        "# Step 1: Use F-score for initial feature ranking\n",
        "# F-statistic measures the linear dependency between each feature and target\n",
        "selector = SelectKBest(score_func=f_regression, k='all')\n",
        "selector.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = pd.DataFrame({\n",
        "    'feature': feature_names_after_preprocessing,\n",
        "    'f_score': selector.scores_\n",
        "}).sort_values('f_score', ascending=False)\n",
        "\n",
        "# Visualize top features\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_n = 20\n",
        "feature_scores.head(top_n).plot(x='feature', y='f_score', kind='barh')\n",
        "plt.xlabel('F-score')\n",
        "plt.title(f'Top {top_n} Features by F-score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTop 15 features by F-score:\")\n",
        "print(feature_scores.head(15))\n",
        "\n",
        "# Step 2: Perform forward selection on top features\n",
        "# Due to computational constraints, we'll select from top 50 features\n",
        "n_top_features = min(50, X_train_transformed.shape[1])\n",
        "top_indices = feature_scores.head(n_top_features).index.tolist()\n",
        "\n",
        "# Get indices of top features\n",
        "top_feature_mask = np.zeros(X_train_transformed.shape[1], dtype=bool)\n",
        "for idx in top_indices:\n",
        "    top_feature_mask[idx] = True\n",
        "\n",
        "X_train_top = X_train_transformed[:, top_feature_mask]\n",
        "X_val_top = X_val_transformed[:, top_feature_mask]\n",
        "\n",
        "print(f\"\\nPerforming forward selection on top {n_top_features} features...\")\n",
        "\n",
        "# Simplified forward selection using RFE (Recursive Feature Elimination)\n",
        "# RFE is computationally more efficient than full forward selection\n",
        "lr_selector = LinearRegression()\n",
        "n_features_to_select = 20  # Final number of features\n",
        "\n",
        "rfe = RFE(estimator=lr_selector, n_features_to_select=n_features_to_select, step=1)\n",
        "rfe.fit(X_train_top, y_train)\n",
        "\n",
        "# Get selected features\n",
        "selected_feature_indices = np.where(rfe.support_)[0]\n",
        "selected_features = [feature_scores.iloc[i]['feature'] for i in selected_feature_indices]\n",
        "\n",
        "print(f\"\\nForward selection chose {len(selected_features)} features:\")\n",
        "for i, feat in enumerate(selected_features, 1):\n",
        "    print(f\"{i:2d}. {feat}\")\n",
        "\n",
        "# Create final selector for pipeline\n",
        "final_selector = SelectKBest(score_func=f_regression, k=30)  # Select top 30 features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8089384",
      "metadata": {
        "id": "e8089384"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8eb502f",
      "metadata": {
        "id": "e8eb502f"
      },
      "source": [
        "## 8. Model Development\n",
        "\n",
        "### 8.1 Linear Regression (Base Model)\n",
        "\n",
        "The linear regression model (ISLR Chapter 3):\n",
        "\n",
        "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon$$\n",
        "\n",
        "We find $\\hat{\\beta}$ by minimizing the residual sum of squares (RSS):\n",
        "\n",
        "$$RSS = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d597652",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3d597652",
        "outputId": "6b5f8d1a-1d4b-4e5f-b95a-99fcf613ea45"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL DEVELOPMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(y_true, y_pred, model_name, dataset_name):\n",
        "    \"\"\"Calculate and display model performance metrics\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"\\n{model_name} - {dataset_name} Performance:\")\n",
        "    print(f\"  MAE:  {mae:.2f} minutes\")\n",
        "    print(f\"  RMSE: {rmse:.2f} minutes\")\n",
        "    print(f\"  R²:   {r2:.4f}\")\n",
        "    \n",
        "    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n",
        "\n",
        "# Create Linear Regression pipeline\n",
        "print(\"\\n1. LINEAR REGRESSION (Base Model)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('selector', final_selector),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the model\n",
        "print(\"Training Linear Regression model...\")\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred_lr = lr_pipeline.predict(X_train)\n",
        "y_val_pred_lr = lr_pipeline.predict(X_val)\n",
        "\n",
        "# Evaluate\n",
        "train_metrics_lr = evaluate_model(y_train, y_train_pred_lr, \"Linear Regression\", \"Training\")\n",
        "val_metrics_lr = evaluate_model(y_val, y_val_pred_lr, \"Linear Regression\", \"Validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df1b63ee",
      "metadata": {
        "id": "df1b63ee"
      },
      "source": [
        "### 8.2 Residual Analysis for Outlier Detection\n",
        "\n",
        "From lecture slides (Outliers, cross validation, resampling):\n",
        "- \"Plot the residuals (regressions)\"\n",
        "- \"Outliers: Plot the residuals\"\n",
        "- \"The outlier has a studentized residual of 6; typically we expect values between −3 and 3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a912e1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a912e1c",
        "outputId": "030ff839-b331-4009-e46f-0ae0349dcee5"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESIDUAL ANALYSIS FOR OUTLIER DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals_train = y_train - y_train_pred_lr\n",
        "residuals_val = y_val - y_val_pred_lr\n",
        "\n",
        "# Create comprehensive residual plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Residuals vs Fitted Values\n",
        "axes[0, 0].scatter(y_train_pred_lr, residuals_train, alpha=0.5, s=10)\n",
        "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Fitted Values')\n",
        "axes[0, 0].set_ylabel('Residuals')\n",
        "axes[0, 0].set_title('Residuals vs Fitted Values')\n",
        "\n",
        "# Add smoothed line to detect patterns\n",
        "from scipy.signal import savgol_filter\n",
        "sorted_indices = np.argsort(y_train_pred_lr)\n",
        "sorted_fitted = y_train_pred_lr[sorted_indices]\n",
        "sorted_residuals = residuals_train.iloc[sorted_indices]\n",
        "if len(sorted_fitted) > 50:\n",
        "    window_size = min(51, len(sorted_fitted) // 10)\n",
        "    if window_size % 2 == 0:\n",
        "        window_size += 1\n",
        "    smoothed = savgol_filter(sorted_residuals, window_size, 3)\n",
        "    axes[0, 0].plot(sorted_fitted, smoothed, 'g-', linewidth=2, label='Smoothed trend')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "# 2. Q-Q plot for normality check\n",
        "stats.probplot(residuals_train, dist=\"norm\", plot=axes[0, 1])\n",
        "axes[0, 1].set_title('Normal Q-Q Plot')\n",
        "axes[0, 1].get_lines()[1].set_color('red')\n",
        "\n",
        "# 3. Histogram of residuals\n",
        "axes[0, 2].hist(residuals_train, bins=50, edgecolor='black', alpha=0.7, density=True)\n",
        "axes[0, 2].set_xlabel('Residuals')\n",
        "axes[0, 2].set_ylabel('Density')\n",
        "axes[0, 2].set_title('Distribution of Residuals')\n",
        "\n",
        "# Add normal distribution overlay\n",
        "residual_std = np.std(residuals_train)\n",
        "residual_mean = np.mean(residuals_train)\n",
        "x_norm = np.linspace(residual_mean - 4*residual_std, residual_mean + 4*residual_std, 100)\n",
        "axes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f05f226",
      "metadata": {
        "id": "8f05f226"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a783e062",
      "metadata": {
        "id": "a783e062"
      },
      "source": [
        "## References\n",
        "\n",
        "1. James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). *An Introduction to Statistical Learning with Applications in Python* (ISLP). Springer.\n",
        "\n",
        "2. Mayer, M. (2025). *Machine Learning Course Materials*. TH Deggendorf.\n",
        "\n",
        "3. Scikit-learn Documentation. https://scikit-learn.org/\n",
        "\n",
        "4. Deutsche Bahn Delays Dataset. Kaggle. https://www.kaggle.com/datasets/nokkyu/deutsche-bahn-db-delays\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c013de33",
      "metadata": {
        "id": "c013de33"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69babb2d",
      "metadata": {
        "id": "69babb2d"
      },
      "source": [
        "<!-- **Team Contributions:**\n",
        "\n",
        "**Member 1 - Data Engineering & Preprocessing:**\n",
        "- Dataset acquisition and initial exploration\n",
        "- Outlier detection and treatment (IQR and Z-score methods)\n",
        "- Feature engineering (time-based, geographic, station-based features)\n",
        "- Data quality assessment and missing value handling\n",
        "- Creation of preprocessing pipelines\n",
        "\n",
        "**Member 2 - Model Development & Optimization:**\n",
        "- Implementation of baseline and linear models\n",
        "- Ridge and Lasso regression with regularization tuning\n",
        "- Random Forest implementation and hyperparameter optimization\n",
        "- Cross-validation setup and execution\n",
        "- Model persistence and deployment preparation\n",
        "\n",
        "**Member 3 - Evaluation & Visualization:**\n",
        "- Comprehensive EDA and feature relationship analysis\n",
        "- Model evaluation metrics and comparison\n",
        "- Learning curves and validation curves\n",
        "- Residual analysis and diagnostic plots\n",
        "- Final report compilation and recommendations\n",
        "\n",
        "**Collaborative Efforts:**\n",
        "- Problem formulation and approach design\n",
        "- Code review and quality assurance\n",
        "- Presentation preparation\n",
        "- Documentation and commenting -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31cdafd",
      "metadata": {
        "id": "f31cdafd"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
