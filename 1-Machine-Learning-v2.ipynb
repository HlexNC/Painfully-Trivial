{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12693595",
   "metadata": {},
   "source": [
    "# Predicting Deutsche Bahn Train Delays  \n",
    "## A Reproducible Baseline for Supervised Regression\n",
    "\n",
    "**Objective:** Build a supervised regression model to predict train arrival delays (in minutes) for Deutsche Bahn trains using statistical learning methods.\n",
    "\n",
    "**Target Variable:** `arrival_delay_m` - continuous variable representing delay in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc24fe",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "### Package Installation (for Google Colab)\n",
    "Run this cell only if using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if running in Google Colab\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn scipy -q\n",
    "# !pip install --upgrade scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df935b3",
   "metadata": {},
   "source": [
    "### Package Installation (for Local Jupyter Setup)\n",
    "```bash\n",
    "# 1. Install Anaconda from https://www.anaconda.com/download\n",
    "\n",
    "# 2. Create a new conda environment\n",
    "conda create -n ml-project python=3.9 -y\n",
    "\n",
    "# 3. Activate the environment\n",
    "conda activate ml-project\n",
    "\n",
    "# 4. Install required packages\n",
    "conda install -c conda-forge pandas numpy matplotlib seaborn scikit-learn scipy jupyter notebook -y\n",
    "\n",
    "# 5. Launch Jupyter Notebook\n",
    "jupyter notebook\n",
    "\n",
    "# 6. Navigate to your notebook file and open it\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721512ec",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    RandomizedSearchCV, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Additional imports\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea15c97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f97d8",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54294e1b",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "For this project, we're using a Deutsche Bahn train delay dataset. The dataset should contain information about scheduled and actual arrival/departure times, routes, and other relevant features for predicting delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f74e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kagglehub import load_dataset, KaggleDatasetAdapter\n",
    "\n",
    "# Load the Deutsche Bahn delays dataset\n",
    "def load_db_delays() -> pd.DataFrame:\n",
    "    df = load_dataset(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"nokkyu/deutsche-bahn-db-delays\",\n",
    "        \"DBtrainrides.csv\"\n",
    "    )\n",
    "    df[\"departure_plan\"] = pd.to_datetime(df[\"departure_plan\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "df = load_db_delays()\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacd99e",
   "metadata": {},
   "source": [
    "<!-- ```\n",
    "print(df.head())\n",
    "\n",
    "                                  ID line  \\\n",
    "0  1573967790757085557-2407072312-14   20   \n",
    "1    349781417030375472-2407080017-1   18   \n",
    "2  7157250219775883918-2407072120-25    1   \n",
    "3    349781417030375472-2407080017-2   18   \n",
    "4   1983158592123451570-2407080010-3   33   \n",
    "\n",
    "                                                path   eva_nr  category  \\\n",
    "0  Stolberg(Rheinl)Hbf Gl.44|Eschweiler-St.Jöris|...  8000001         2   \n",
    "1                                                NaN  8000001         2   \n",
    "2  Hamm(Westf)Hbf|Kamen|Kamen-Methler|Dortmund-Ku...  8000406         4   \n",
    "3                                         Aachen Hbf  8000404         5   \n",
    "4                            Herzogenrath|Kohlscheid  8000404         5   \n",
    "\n",
    "             station                state    city    zip      long        lat  \\\n",
    "0         Aachen Hbf  Nordrhein-Westfalen  Aachen  52064  6.091499  50.767800   \n",
    "1         Aachen Hbf  Nordrhein-Westfalen  Aachen  52064  6.091499  50.767800   \n",
    "2  Aachen-Rothe Erde  Nordrhein-Westfalen  Aachen  52066  6.116475  50.770202   \n",
    "3        Aachen West  Nordrhein-Westfalen  Aachen  52072  6.070715  50.780360   \n",
    "4        Aachen West  Nordrhein-Westfalen  Aachen  52072  6.070715  50.780360   \n",
    "\n",
    "          arrival_plan       departure_plan       arrival_change  \\\n",
    "0  2024-07-08 00:00:00  2024-07-08 00:01:00  2024-07-08 00:03:00   \n",
    "1                  NaN  2024-07-08 00:17:00                  NaN   \n",
    "2  2024-07-08 00:03:00  2024-07-08 00:04:00  2024-07-08 00:03:00   \n",
    "3  2024-07-08 00:20:00  2024-07-08 00:21:00                  NaN   \n",
    "4  2024-07-08 00:20:00  2024-07-08 00:21:00  2024-07-08 00:20:00   \n",
    "\n",
    "      departure_change  arrival_delay_m  departure_delay_m info  \\\n",
    "0  2024-07-08 00:04:00                3                  3  NaN   \n",
    "1                  NaN                0                  0  NaN   \n",
    "2  2024-07-08 00:04:00                0                  0  NaN   \n",
    "3                  NaN                0                  0  NaN   \n",
    "4  2024-07-08 00:21:00                0                  0  NaN   \n",
    "\n",
    "  arrival_delay_check departure_delay_check  \n",
    "0             on_time               on_time  \n",
    "1             on_time               on_time  \n",
    "2             on_time               on_time  \n",
    "3             on_time               on_time  \n",
    "4             on_time               on_time  \n",
    "``` -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8957a59",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "Following ISLP Section 2.3.9 - Additional Graphical and Numerical Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f996b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\\nFirst 5 rows:\")\n",
    "print(\"=\"*50)\n",
    "df.head()\n",
    "\n",
    "print(\"\\n\\nBasic Statistics:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()\n",
    "\n",
    "print(\"\\n\\nMissing Values:\")\n",
    "print(\"=\"*50)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cdecd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371a4f5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869a8d7",
   "metadata": {},
   "source": [
    "### Target Variable Analysis\n",
    "\n",
    "Understanding the distribution of our target variable is crucial for model selection and evaluation metrics.\n",
    "\n",
    "**Mathematical Framework (ISLP Equation 2.1):**\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $Y$ is the response variable (arrival_delay_m)\n",
    "- $X$ represents our predictors\n",
    "- $f$ is the systematic information\n",
    "- $\\epsilon$ is random error with $E(\\epsilon) = 0$ and $Var(\\epsilon) = \\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive target analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Histogram of arrival delays\n",
    "axes[0, 0].hist(df['arrival_delay_m'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Arrival Delay (minutes)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Arrival Delays')\n",
    "axes[0, 0].axvline(df['arrival_delay_m'].mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {df[\"arrival_delay_m\"].mean():.1f} min')\n",
    "axes[0, 0].axvline(df['arrival_delay_m'].median(), color='green', linestyle='--', \n",
    "                    label=f'Median: {df[\"arrival_delay_m\"].median():.1f} min')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Box plot to identify outliers\n",
    "axes[0, 1].boxplot(df['arrival_delay_m'].dropna())\n",
    "axes[0, 1].set_ylabel('Arrival Delay (minutes)')\n",
    "axes[0, 1].set_title('Box Plot of Arrival Delays')\n",
    "\n",
    "# 3. Q-Q plot for normality check\n",
    "stats.probplot(df['arrival_delay_m'].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot: Checking Normality')\n",
    "\n",
    "# 4. Log-transformed delays\n",
    "delay_positive = df['arrival_delay_m'] + 1  # Add 1 to handle zeros\n",
    "axes[1, 1].hist(np.log(delay_positive.dropna()), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Log(Arrival Delay + 1)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Log-Transformed Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nTarget Variable Statistical Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean delay: {df['arrival_delay_m'].mean():.2f} minutes\")\n",
    "print(f\"Median delay: {df['arrival_delay_m'].median():.2f} minutes\")\n",
    "print(f\"Standard deviation: {df['arrival_delay_m'].std():.2f} minutes\")\n",
    "print(f\"Skewness: {df['arrival_delay_m'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df['arrival_delay_m'].kurtosis():.2f}\")\n",
    "print(f\"Percentage of on-time arrivals: {(df['arrival_delay_m'] == 0).sum() / len(df) * 100:.1f}%\")\n",
    "print(f\"95th percentile: {df['arrival_delay_m'].quantile(0.95):.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69504cdd",
   "metadata": {},
   "source": [
    "### Feature Analysis and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between features and target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Delay by train type\n",
    "df.boxplot(column='arrival_delay_m', by='train_type', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Delays by Train Type')\n",
    "\n",
    "# 2. Delay by hour of day\n",
    "hourly_delays = df.groupby('hour')['arrival_delay_m'].mean()\n",
    "axes[0, 1].plot(hourly_delays.index, hourly_delays.values, marker='o')\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].set_ylabel('Average Delay (min)')\n",
    "axes[0, 1].set_title('Average Delays by Hour')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. Delay by day of week\n",
    "daily_delays = df.groupby('day_of_week')['arrival_delay_m'].mean()\n",
    "axes[0, 2].bar(daily_delays.index, daily_delays.values)\n",
    "axes[0, 2].set_xlabel('Day of Week (0=Monday)')\n",
    "axes[0, 2].set_ylabel('Average Delay (min)')\n",
    "axes[0, 2].set_title('Average Delays by Day of Week')\n",
    "\n",
    "# 4. Delay vs distance\n",
    "axes[1, 0].scatter(df['distance_km'], df['arrival_delay_m'], alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Distance (km)')\n",
    "axes[1, 0].set_ylabel('Delay (min)')\n",
    "axes[1, 0].set_title('Delay vs Distance')\n",
    "\n",
    "# 5. Delay by weather condition\n",
    "df.boxplot(column='arrival_delay_m', by='weather_condition', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Delays by Weather Condition')\n",
    "\n",
    "# 6. Correlation heatmap for numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347077b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ca113",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a2112",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split\n",
    "\n",
    "Following ISLP Section 5.1 - Cross-Validation principles, we split our data into three sets:\n",
    "- Training set (60%): For model fitting\n",
    "- Validation set (20%): For hyperparameter tuning\n",
    "- Test set (20%): For final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing target values\n",
    "df_clean = df.dropna(subset=['arrival_delay_m']).copy()\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop('arrival_delay_m', axis=1)\n",
    "y = df_clean['arrival_delay_m']\n",
    "\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 of 0.8 = 0.2\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f4b37",
   "metadata": {},
   "source": [
    "### Feature Engineering and Preprocessing Pipeline\n",
    "\n",
    "Reference: ISLP Section 6.2 - Ridge Regression and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b94d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), \n",
    "         categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor on training data only\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Get feature names after transformation\n",
    "feature_names = (numerical_features + \n",
    "                [f\"{cat}_{val}\" for cat, vals in \n",
    "                 zip(categorical_features, preprocessor.named_transformers_['cat'].categories_) \n",
    "                 for val in vals[1:]])  # drop='first' removes first category\n",
    "\n",
    "print(f\"\\nTotal features after preprocessing: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16ebab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f2ea3",
   "metadata": {},
   "source": [
    "## 5. Model Development and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85760eb",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "Following the project requirements, we start with a baseline model that predicts the mean delay.\n",
    "This provides a performance floor for comparison.\n",
    "\n",
    "Mathematical Foundation: The simplest model predicts the mean:\n",
    "$$\\hat{y} = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Mean predictor\n",
    "mean_delay = y_train.mean()\n",
    "baseline_train_pred = np.full(len(y_train), mean_delay)\n",
    "baseline_val_pred = np.full(len(y_val), mean_delay)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"MAE: {mae:.2f} minutes\")\n",
    "    print(f\"RMSE: {rmse:.2f} minutes\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    \n",
    "    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n",
    "\n",
    "baseline_train_metrics = evaluate_model(y_train, baseline_train_pred, \"Baseline (Mean) - Training\")\n",
    "baseline_val_metrics = evaluate_model(y_val, baseline_val_pred, \"Baseline (Mean) - Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc1490",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "**Mathematical Foundation (ISLP Chapter 3):**\n",
    "Linear regression assumes:\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p + \\epsilon$$\n",
    "\n",
    "The coefficients are estimated by minimizing RSS:\n",
    "$$\\text{RSS} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_pipeline.predict(X_train)\n",
    "y_val_pred_lr = lr_pipeline.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "lr_train_metrics = evaluate_model(y_train, y_train_pred_lr, \"Linear Regression - Training\")\n",
    "lr_val_metrics = evaluate_model(y_val, y_val_pred_lr, \"Linear Regression - Validation\")\n",
    "\n",
    "# Residual analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_val - y_val_pred_lr\n",
    "axes[0].scatter(y_val_pred_lr, residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot - Linear Regression')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[1].scatter(y_val, y_val_pred_lr, alpha=0.5)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')\n",
    "axes[1].set_xlabel('Actual Delays')\n",
    "axes[1].set_ylabel('Predicted Delays')\n",
    "axes[1].set_title('Actual vs Predicted - Linear Regression')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46324fd4",
   "metadata": {},
   "source": [
    "### Advanced Model: Random Forest with Hyperparameter Optimization\n",
    "\n",
    "**Mathematical Foundation (ISLP Section 8.2):**\n",
    "Random Forest combines multiple decision trees:\n",
    "$$\\hat{f}(x) = \\frac{1}{B}\\sum_{b=1}^{B} T_b(x)$$\n",
    "\n",
    "where $T_b$ is the $b$-th tree trained on a bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3867e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with initial parameters\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [10, 20, 30, None],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],\n",
    "    'regressor__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Number of parameter combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Random Forest hyperparameter optimization...\")\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_random_search.best_params_}\")\n",
    "print(f\"Best CV score (MAE): {-rf_random_search.best_score_:.2f}\")\n",
    "\n",
    "# Use best model\n",
    "rf_best = rf_random_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_best.predict(X_train)\n",
    "y_val_pred_rf = rf_best.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "rf_train_metrics = evaluate_model(y_train, y_train_pred_rf, \"Random Forest (Optimized) - Training\")\n",
    "rf_val_metrics = evaluate_model(y_val, y_val_pred_rf, \"Random Forest (Optimized) - Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa1618",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6871c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from Random Forest\n",
    "rf_model = rf_best.named_steps['regressor']\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Most Important Features - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cff82",
   "metadata": {},
   "source": [
    "### Learning Curves Analysis\n",
    "\n",
    "Reference: ISLP Section 5.1 - Understanding model performance across different training sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(estimator, title, X, y, cv=5):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    train_scores_std = train_scores.std(axis=1)\n",
    "    val_scores_mean = -val_scores.mean(axis=1)\n",
    "    val_scores_std = val_scores.std(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(f\"Learning Curves - {title}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for both models\n",
    "plot_learning_curves(lr_pipeline, \"Linear Regression\", X_train, y_train)\n",
    "plot_learning_curves(rf_best, \"Random Forest (Optimized)\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3b02e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde9a11",
   "metadata": {},
   "source": [
    "## 6. Final Model Selection and Test Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754063cf",
   "metadata": {},
   "source": [
    "### Model Comparison and Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "val_performances = {\n",
    "    'Baseline (Mean)': baseline_val_metrics['mae'],\n",
    "    'Linear Regression': lr_val_metrics['mae'],\n",
    "    'Random Forest': rf_val_metrics['mae']\n",
    "}\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = min(val_performances, key=val_performances.get)\n",
    "print(f\"\\nBest model based on validation MAE: {best_model_name}\")\n",
    "print(f\"Validation MAE: {val_performances[best_model_name]:.2f} minutes\")\n",
    "\n",
    "# Model comparison visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = list(val_performances.keys())\n",
    "mae_values = list(val_performances.values())\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "bars = plt.bar(models, mae_values, color=colors, alpha=0.7)\n",
    "plt.ylabel('MAE (minutes)')\n",
    "plt.title('Model Performance Comparison (Validation Set)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, mae_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{value:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042975d",
   "metadata": {},
   "source": [
    "### Final Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01742218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on combined train+validation set\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "# Select and retrain best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    final_model = rf_best\n",
    "elif best_model_name == 'Linear Regression':\n",
    "    final_model = lr_pipeline\n",
    "else:\n",
    "    # Baseline doesn't need retraining\n",
    "    final_model = None\n",
    "\n",
    "if final_model is not None:\n",
    "    final_model.fit(X_train_full, y_train_full)\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "else:\n",
    "    y_test_pred = np.full(len(y_test), y_train_full.mean())\n",
    "\n",
    "# Final evaluation\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, f\"{best_model_name} - Test Set\")\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[0, 0].set_xlabel('Actual Delays')\n",
    "axes[0, 0].set_ylabel('Predicted Delays')\n",
    "axes[0, 0].set_title(f'Test Set: Actual vs Predicted - {best_model_name}')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Residual distribution\n",
    "residuals = y_test - y_test_pred\n",
    "axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Prediction Error (minutes)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Prediction Errors')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# 3. Residuals vs Predicted\n",
    "axes[1, 0].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Predicted Values')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residual Plot')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 4. Performance metrics summary\n",
    "metrics_text = f\"\"\"Final Model: {best_model_name}\n",
    "\n",
    "Test Set Performance:\n",
    "MAE: {test_metrics['mae']:.2f} minutes\n",
    "RMSE: {test_metrics['rmse']:.2f} minutes\n",
    "R²: {test_metrics['r2']:.4f}\n",
    "\n",
    "Validation Performance:\n",
    "MAE: {val_performances[best_model_name]:.2f} minutes\n",
    "\n",
    "Baseline MAE: {baseline_val_metrics['mae']:.2f} minutes\n",
    "Improvement: {(baseline_val_metrics['mae'] - test_metrics['mae'])/baseline_val_metrics['mae']*100:.1f}%\n",
    "\n",
    "Mean Absolute % Error: {(np.abs(residuals) / (y_test + 1)).mean() * 100:.1f}%\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes,\n",
    "                fontsize=12, verticalalignment='center', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\"))\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8089384",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb502f",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb70a10",
   "metadata": {},
   "source": [
    "### Project Summary\n",
    "\n",
    "This analysis demonstrates a complete machine learning workflow for predicting train delays, following the project work rules and ISLP methodology:\n",
    "\n",
    "1. **Data Exploration**: We identified that train delays follow a right-skewed distribution with significant outliers.\n",
    "\n",
    "2. **Data Preprocessing**: Implemented proper train-validation-test splits and feature engineering with standardization for numerical features and one-hot encoding for categorical features.\n",
    "\n",
    "3. **Model Development**: \n",
    "   - Baseline model (mean predictor) for performance floor\n",
    "   - Linear Regression as a simple interpretable model\n",
    "   - Random Forest with hyperparameter optimization as an advanced model\n",
    "\n",
    "4. **Evaluation**: Used appropriate metrics (MAE, RMSE, R²) and visualizations to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d597652",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL PROJECT RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test Set MAE: {test_metrics['mae']:.2f} minutes\")\n",
    "print(f\"Improvement over baseline: {(baseline_val_metrics['mae'] - test_metrics['mae'])/baseline_val_metrics['mae']*100:.1f}%\")\n",
    "print(f\"\\nThe model can predict train delays with an average error of {test_metrics['mae']:.2f} minutes.\")\n",
    "\n",
    "# Save model performance summary\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Linear Regression', 'Random Forest'],\n",
    "    'Validation MAE': [baseline_val_metrics['mae'], lr_val_metrics['mae'], rf_val_metrics['mae']],\n",
    "    'Test MAE': [baseline_val_metrics['mae'], \n",
    "                 test_metrics['mae'] if best_model_name == 'Linear Regression' else np.nan,\n",
    "                 test_metrics['mae'] if best_model_name == 'Random Forest' else np.nan]\n",
    "})\n",
    "\n",
    "print(\"\\n\\nModel Performance Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783e062",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). *An Introduction to Statistical Learning with Applications in Python* (ISLP)\n",
    "- Mayer, M. (2025). *Machine Learning Course Materials*, TH Deggendorf\n",
    "- Scikit-learn Documentation: https://scikit-learn.org/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
